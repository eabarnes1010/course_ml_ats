{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltiRUAOKhme7"
   },
   "source": [
    "# Comparing methods of hurricane forecast uncertainty\n",
    "[![Latest release](https://badgen.net/github/release/Naereen/Strapdown.js)](https://github.com/eabarnes1010/course_ml_ats/tree/main/code)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eabarnes1010/course_ml_ats/blob/main/code/ann_uq_hurricanes.ipynb)\n",
    "\n",
    "_Ongoing research by Elizabeth A. Barnes, Randal J. Barnes and Mark DeMaria_\n",
    "\n",
    "We will be forecasting the error of the Consensus forecast (mean across multiple hurricane models) of hurricane intensity 72 hours in advance for hurricanes over the Eastern Pacific / Central Pacific. That is, our network will be used as a post-processing technique. Our input features (predictors) are composed of 11 variables that include information we think might be helpful, e.g. the models that went into the Consensus forecast, the environment (e.g. sea-surface temperature, shear), and the current latitude of the storm.\n",
    "\n",
    "We will be predicting parameters of the conditional SHASH distribution for each prediction and then plotting these distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2Vz-7EXrFcF"
   },
   "source": [
    "## Initial import and module statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1645453433829,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "DzlfH1ZSjVVN",
    "outputId": "e7d78424-ecd1-4f2f-af40-67fa592a23c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB = False\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "print('IN_COLAB = ' + str(IN_COLAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5633,
     "status": "ok",
     "timestamp": 1645453439540,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "FulYOApOhmfA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn import preprocessing\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import optimizers\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numpy.random import default_rng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1645453439541,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "NK2UKxkEhmfC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"figure.dpi\"] = 150\n",
    "np.warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1645453439542,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "xuLVNbXAhmfC",
    "outputId": "4f8cff34-bf0f-45c5-ff1b-d4a67cce26c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version = 3.7.11 (default, Jul 27 2021, 07:03:16) \n",
      "[Clang 10.0.0 ]\n",
      "numpy version = 1.21.5\n",
      "tensorflow version = 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"python version = {sys.version}\")\n",
    "print(f\"numpy version = {np.__version__}\")\n",
    "print(f\"tensorflow version = {tf.__version__}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9HL7n3Sjmaf",
    "tags": []
   },
   "source": [
    "## Lots of predefined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1olTfiOhqKPg",
    "tags": []
   },
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1645453440097,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "MP9ZF6Ozh7yy"
   },
   "outputs": [],
   "source": [
    "def plot_history(history, model_name):\n",
    "    \"\"\"Plot the model.fit training history and save the resulting figure.\n",
    "\n",
    "    Creates a 2-by-2 block of subplots.  The four plots are:\n",
    "        1 -- training and validations loss history.\n",
    "        2 -- training and validations customMAE history.\n",
    "        3 -- training and validations InterquartileCapture history.\n",
    "        4 -- training and validations SignTest history.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    history : tf.keras.callbacks.History\n",
    "        The history must have at least the following eight items\n",
    "        in the history.history.keys()\n",
    "            \"loss\",\n",
    "            \"val_loss\",\n",
    "            \"custom_mae\",\n",
    "            \"val_custom_mae\",\n",
    "            \"interquartile_capture\",\n",
    "            \"val_interquartile_capture\",\n",
    "            \"sign_test\",\n",
    "            \"val_sign_test\"\n",
    "\n",
    "    model_name : str\n",
    "        The resulting figure is saved to:\n",
    "            \"figures/model_diagnostics/\" + model_name + \".png\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    TRAIN_COLOR = \"#7570b3\"\n",
    "    VALID_COLOR = \"#e7298a\"\n",
    "    FIGSIZE = (14, 10)\n",
    "    FONTSIZE = 12\n",
    "    DPIFIG = 300.0\n",
    "\n",
    "    best_epoch = np.argmin(history.history[\"val_loss\"])\n",
    "\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "\n",
    "    # Plot the training and validations loss history.\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(\n",
    "        history.history[\"loss\"],\n",
    "        \"o\",\n",
    "        color=TRAIN_COLOR,\n",
    "        markersize=3,\n",
    "        label=\"train\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        history.history[\"val_loss\"],\n",
    "        \"o\",\n",
    "        color=VALID_COLOR,\n",
    "        markersize=3,\n",
    "        label=\"valid\",\n",
    "    )\n",
    "\n",
    "    plt.axvline(x=best_epoch, linestyle=\"--\", color=\"tab:gray\")\n",
    "    plt.title(\"Log-likelihood Loss Function\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(frameon=True, fontsize=FONTSIZE)\n",
    "\n",
    "    # Plot the training and validations customMAE history.\n",
    "    try:\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(\n",
    "            history.history[\"custom_mae\"],\n",
    "            \"o\",\n",
    "            color=TRAIN_COLOR,\n",
    "            markersize=3,\n",
    "            label=\"train\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            history.history[\"val_custom_mae\"],\n",
    "            \"o\",\n",
    "            color=VALID_COLOR,\n",
    "            markersize=3,\n",
    "            label=\"valid\",\n",
    "        )\n",
    "\n",
    "        plt.axvline(x=best_epoch, linestyle=\"--\", color=\"tab:gray\")\n",
    "        plt.title(\"Mean |true - median|\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.grid(True)\n",
    "        plt.legend(frameon=True, fontsize=FONTSIZE)\n",
    "    except:\n",
    "        print('no mae metric, skipping plot')\n",
    "        \n",
    "    # Plot the training and validations InterquartileCapture history.\n",
    "    try:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(\n",
    "            history.history[\"interquartile_capture\"],\n",
    "            \"o\",\n",
    "            color=TRAIN_COLOR,\n",
    "            markersize=3,\n",
    "            label=\"train\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            history.history[\"val_interquartile_capture\"],\n",
    "            \"o\",\n",
    "            color=VALID_COLOR,\n",
    "            markersize=3,\n",
    "            label=\"valid\",\n",
    "        )\n",
    "\n",
    "        plt.axvline(x=best_epoch, linestyle=\"--\", color=\"tab:gray\")\n",
    "        plt.title(\"Fraction Between 25 and 75 Percentile\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.grid(True)\n",
    "        plt.legend(frameon=True, fontsize=FONTSIZE)\n",
    "    except:\n",
    "        print('no interquartile_capture, skipping plot')\n",
    "\n",
    "    # Plot the training and validations SignTest history.\n",
    "    try:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(\n",
    "            history.history[\"sign_test\"],\n",
    "            \"o\",\n",
    "            color=TRAIN_COLOR,\n",
    "            markersize=3,\n",
    "            label=\"train\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            history.history[\"val_sign_test\"],\n",
    "            \"o\",\n",
    "            color=VALID_COLOR,\n",
    "            markersize=3,\n",
    "            label=\"valid\",\n",
    "        )\n",
    "\n",
    "        plt.axvline(x=best_epoch, linestyle=\"--\", color=\"tab:gray\")\n",
    "        plt.title(\"Fraction Above the Median\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.grid(True)\n",
    "        plt.legend(frameon=True, fontsize=FONTSIZE)\n",
    "    except:\n",
    "        print('no sign-test, skipping plot')\n",
    "\n",
    "    # Draw and save the plot.\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"figures/model_diagnostics/\" + model_name + \".png\", dpi=DPIFIG)\n",
    "    plt.show()\n",
    "\n",
    "class TrainingInstrumentation(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Plot real-time training instrumentation panel.\n",
    "\n",
    "    If x_data and onehot_data are not given, the instrumentation panel\n",
    "    includes only the real-time plot of the training and validation loss.\n",
    "\n",
    "    If the x_data and onehot_data are given, the instrumentation panel also\n",
    "    includes the PIT histogram plot, and histogram plots for each of the\n",
    "    local conditional distribution parameters, updated in real time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data : tensor, default=None\n",
    "        The x_train (or x_valid) tensor.  If either x_data or onehot_data\n",
    "        is specifed, then both must be specified and they must have the\n",
    "        same number of rows.\n",
    "\n",
    "    onehot_data : tensor, default=None\n",
    "        The onehot_train (or onehot_valid) tensor. If either x_data or\n",
    "        onehot_data is specifed, then both must be specified and they must\n",
    "        have the same number of rows.\n",
    "\n",
    "    figsize: (float, float), default=(13, 7)\n",
    "        Size of the instrumentation panel.\n",
    "\n",
    "    interval: int, default=1\n",
    "        Number of epochs (steps) between refreshing the instruments.  By\n",
    "        default, interval=1, and the intruments are updated ever epoch.\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    * Include TrainingInstrumentation() as a callback in model.fit; e.g.\n",
    "\n",
    "        training_callback = TrainingInstrumentation(\n",
    "            x_train_std, onehot_train, interval=10\n",
    "        )\n",
    "        ...\n",
    "        history = model.fit(\n",
    "            ...\n",
    "            callbacks=[training_callback],\n",
    "        )\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * This Class is explcitly designed for the SHASH distribution, with\n",
    "        parameter names 'mu', 'sigma', 'gamma', and 'tau'.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_data=None,\n",
    "        onehot_data=None,\n",
    "        figsize=(13, 7),\n",
    "        interval=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.x_data = x_data\n",
    "        self.onehot_data = onehot_data\n",
    "        self.figsize = figsize\n",
    "        self.interval = interval\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.loss.append(logs.get(\"loss\"))\n",
    "        self.val_loss.append(logs.get(\"val_loss\"))\n",
    "\n",
    "        if epoch % self.interval == 0:\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=self.figsize)\n",
    "\n",
    "            best_epoch = np.argmin(self.val_loss)\n",
    "            plt.subplot(3, 2, 1)\n",
    "            plt.plot(self.loss, \"o\", color=\"#7570b3\", label=\"train\", markersize=2)\n",
    "            plt.plot(self.val_loss, \"o\", color=\"#e7298a\", label=\"valid\", markersize=2)\n",
    "            plt.axvline(x=best_epoch, linestyle=\"--\", color=\"gray\")\n",
    "            plt.title(f\"Loss After {epoch} Epochs\")\n",
    "            plt.grid(True)\n",
    "            plt.legend(\n",
    "                [\n",
    "                    f\"train = {logs.get('loss'):.3f}\",\n",
    "                    f\"valid = {logs.get('val_loss'):.3f}\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if (self.x_data is not None) and (self.onehot_data is not None):\n",
    "                preds = self.model.predict(self.x_data)\n",
    "\n",
    "                if preds.shape[1] >= 1:\n",
    "                    mu = preds[:, 0]\n",
    "                    plt.subplot(3, 2, 3)\n",
    "                    plt.hist(mu, bins=30, color=\"#7fc97f\", edgecolor=\"k\")\n",
    "                    plt.legend([\"mu\"])\n",
    "\n",
    "                if preds.shape[1] >= 2:\n",
    "                    # sigma = tf.math.exp(preds[:, 1])\n",
    "                    sigma = preds[:, 1]\n",
    "                    plt.subplot(3, 2, 4)\n",
    "                    plt.hist(sigma, bins=30, color=\"#beaed4\", edgecolor=\"k\")\n",
    "                    plt.legend([\"sigma\"])\n",
    "                else:\n",
    "                    sigma = tf.zeros_like(mu)\n",
    "\n",
    "                if preds.shape[1] >= 3:\n",
    "                    gamma = preds[:, 2]\n",
    "                    plt.subplot(3, 2, 5)\n",
    "                    plt.hist(gamma, bins=30, color=\"#fdc086\", edgecolor=\"k\")\n",
    "                    plt.legend([\"gamma\"])\n",
    "                else:\n",
    "                    gamma = tf.zeros_like(mu)\n",
    "\n",
    "                if preds.shape[1] >= 4:\n",
    "                    # tau = tf.math.exp(preds[:, 3])\n",
    "                    tau = preds[:, 3]\n",
    "                    plt.subplot(3, 2, 6)\n",
    "                    plt.hist(tau, bins=30, color=\"#ffff99\", edgecolor=\"k\")\n",
    "                    plt.legend([\"tau\"])\n",
    "                else:\n",
    "                    tau = tf.ones_like(mu)\n",
    "\n",
    "                F = shash_cdf(self.onehot_data[:, 0], mu, sigma, gamma, tau)\n",
    "                plt.subplot(3, 2, 2)\n",
    "                plt.hist(\n",
    "                    F.numpy(),\n",
    "                    bins=np.linspace(0, 1, 21),\n",
    "                    color=\"#386cb0\",\n",
    "                    edgecolor=\"k\",\n",
    "                )\n",
    "                plt.legend([\"PIT\"])\n",
    "                plt.axhline(y=F.shape[0] / 20, color=\"b\", linestyle=\"--\")\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1645453440097,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "t6C0CvcGhmfE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def params(x_inputs, model):\n",
    "    \"\"\"Funtion to make shash parameter predictions for shash2, shash3, shash4\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    x_inputs : floats\n",
    "        matrix of inputs to the network\n",
    "\n",
    "    model : tensorflow model\n",
    "        neural network for predictions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vector of predicted parameters\n",
    "\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(x_inputs)\n",
    "    mu_pred = y_pred[:, 0]\n",
    "    sigma_pred = y_pred[:, 1]\n",
    "\n",
    "    gamma_pred = np.zeros(np.shape(y_pred[:, 0]),dtype='float32')\n",
    "    if np.shape(y_pred)[1] == 3:\n",
    "        gamma_pred = y_pred[:, 2]\n",
    "\n",
    "    tau_pred = np.ones(np.shape(y_pred[:, 0]),dtype='float32')\n",
    "    if np.shape(y_pred)[1] == 4:\n",
    "        tau_pred = y_pred[:, 3]\n",
    "\n",
    "    return mu_pred, sigma_pred, gamma_pred, tau_pred\n",
    "\n",
    "\n",
    "def percentile_value(mu_pred, sigma_pred, gamma_pred, tau_pred, percentile_frac=0.5):\n",
    "    \"\"\"Function to obtain percentile value of the shash distribution.\"\"\"\n",
    "    return shash_quantile(\n",
    "        pr=percentile_frac, mu=mu_pred, sigma=sigma_pred, gamma=gamma_pred, tau=tau_pred\n",
    "    ).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmrETbrpkt6e",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics and loss functions / scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1014,
     "status": "ok",
     "timestamp": 1645453441036,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "BkTWaukVhmfF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Exponentiate(keras.layers.Layer):\n",
    "    \"\"\"Custom layer to exp the sigma and tau estimates inline.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Exponentiate, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.exp(inputs)\n",
    "\n",
    "class InterquartileCapture(tf.keras.metrics.Metric):\n",
    "    \"\"\"Compute the fraction of true values between the 25 and 75 percentiles.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, pred, sample_weight=None):\n",
    "        mu = pred[:, 0]\n",
    "        sigma = pred[:, 1]\n",
    "\n",
    "        if pred.shape[1] >= 3:\n",
    "            gamma = pred[:, 2]\n",
    "        else:\n",
    "            gamma = tf.zeros_like(mu)\n",
    "\n",
    "        if pred.shape[1] >= 4:\n",
    "            tau = pred[:, 3]\n",
    "        else:\n",
    "            tau = tf.ones_like(mu)\n",
    "\n",
    "        lower = shash_quantile(0.25, mu, sigma, gamma, tau)\n",
    "        upper = shash_quantile(0.75, mu, sigma, gamma, tau)\n",
    "\n",
    "        batch_count = tf.reduce_sum(\n",
    "            tf.cast(\n",
    "                tf.math.logical_and(\n",
    "                    tf.math.greater(y_true[:, 0], lower),\n",
    "                    tf.math.less(y_true[:, 0], upper)\n",
    "                ),\n",
    "                tf.float32\n",
    "            )\n",
    "\n",
    "        )\n",
    "        batch_total = len(y_true[:, 0])\n",
    "\n",
    "        self.count.assign_add(tf.cast(batch_count, tf.float32))\n",
    "        self.total.assign_add(tf.cast(batch_total, tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.count / self.total\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config}\n",
    "\n",
    "\n",
    "class SignTest(tf.keras.metrics.Metric):\n",
    "    \"\"\"Compute the fraction of true values above the median.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, pred, sample_weight=None):\n",
    "        mu = pred[:, 0]\n",
    "        sigma = pred[:, 1]\n",
    "\n",
    "        if pred.shape[1] >= 3:\n",
    "            gamma = pred[:, 2]\n",
    "        else:\n",
    "            gamma = tf.zeros_like(mu)\n",
    "\n",
    "        if pred.shape[1] >= 4:\n",
    "            tau = pred[:, 3]\n",
    "        else:\n",
    "            tau = tf.ones_like(mu)\n",
    "\n",
    "        median = shash_median(mu, sigma, gamma, tau)\n",
    "\n",
    "        batch_count = tf.reduce_sum(\n",
    "            tf.cast(tf.math.greater(y_true[:, 0], median), tf.float32)\n",
    "        )\n",
    "        batch_total = len(y_true[:, 0])\n",
    "\n",
    "        self.count.assign_add(tf.cast(batch_count, tf.float32))\n",
    "        self.total.assign_add(tf.cast(batch_total, tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.count / self.total\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config}\n",
    "\n",
    "    \n",
    "class CustomMAE(tf.keras.metrics.Metric):\n",
    "    \"\"\"Compute the prediction mean absolute error.\n",
    "\n",
    "    The \"predicted value\" is the median of the conditional distribution.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * The computation is done by maintaining running sums of total predictions\n",
    "        and correct predictions made across all batches in an epoch. The\n",
    "        running sums are reset at the end of each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.error = self.add_weight(\"error\", initializer=\"zeros\")\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, pred, sample_weight=None):\n",
    "        mu = pred[:, 0]\n",
    "        sigma = pred[:, 1]\n",
    "\n",
    "        if pred.shape[1] >= 3:\n",
    "            gamma = pred[:, 2]\n",
    "        else:\n",
    "            gamma = tf.zeros_like(mu)\n",
    "\n",
    "        if pred.shape[1] >= 4:\n",
    "            tau = pred[:, 3]\n",
    "        else:\n",
    "            tau = tf.ones_like(mu)\n",
    "\n",
    "        predictions = shash_median(mu, sigma, gamma, tau)\n",
    "\n",
    "        error = tf.math.abs(y_true[:, 0] - predictions)\n",
    "        batch_error = tf.reduce_sum(error)\n",
    "        batch_total = tf.math.count_nonzero(error)\n",
    "\n",
    "        self.error.assign_add(tf.cast(batch_error, tf.float32))\n",
    "        self.total.assign_add(tf.cast(batch_total, tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.error / self.total\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config}\n",
    "   \n",
    "    \n",
    "def compute_NLL(y, distr): \n",
    "    return -distr.log_prob(y) \n",
    "\n",
    "def compute_shash_NLL(y_true, pred):\n",
    "    \"\"\"Negative log-likelihood loss using the sinh-arcsinh normal distribution.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    y_true : tensor\n",
    "        The ground truth values.\n",
    "        shape = [batch_size, n_parameter]\n",
    "\n",
    "    pred :\n",
    "        The predicted local conditionsal distribution parameter values.\n",
    "        shape = [batch_size, n_parameters]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : tensor, shape = [1, 1]\n",
    "        The average negative log-likelihood of the batch using the predicted\n",
    "        conditional distribution parameters.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * The value of n_parameters depends on the chosen form of the conditional\n",
    "        sinh-arcsinh normal distribution.\n",
    "            shash2 -> n_parameter = 2, i.e. mu, sigma\n",
    "            shash3 -> n_parameter = 3, i.e. mu, sigma, gamma\n",
    "            shash4 -> n_parameter = 4, i.e. mu, sigma, gamma, tau\n",
    "\n",
    "    * Since sigma and tau must be strictly positive, the network learns the\n",
    "        log of these two parameters.\n",
    "\n",
    "    * If gamma is not learned (i.e. shash2), they are set to 0.\n",
    "\n",
    "    * If tau is not learned (i.e. shash2 or shash3), they are set to 1.\n",
    "\n",
    "    \"\"\"\n",
    "    mu = pred[:, 0]\n",
    "    sigma = pred[:, 1]\n",
    "\n",
    "    if pred.shape[1] >= 3:\n",
    "        gamma = pred[:, 2]\n",
    "    else:\n",
    "        gamma = tf.zeros_like(mu)\n",
    "\n",
    "    if pred.shape[1] >= 4:\n",
    "        tau = pred[:, 3]\n",
    "    else:\n",
    "        tau = tf.ones_like(mu)\n",
    "\n",
    "    loss = -shash_log_prob(y_true[:, 0], mu, sigma, gamma, tau)\n",
    "    return tf.reduce_mean(loss, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3V0tnCQYkcrS",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Define and compute the SHASH distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1645453441543,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "GvGdDIhJilKx"
   },
   "outputs": [],
   "source": [
    "\"\"\"sinh-arcsinh normal distribution w/o using tensorflow_probability.\n",
    "\n",
    "Functions\n",
    "---------\n",
    "cdf(x, mu, sigma, gamma, tau=None)\n",
    "    cumulative distribution function (cdf).\n",
    "\n",
    "log_prob(x, mu, sigma, gamma, tau=None)\n",
    "    log of the probability density function.\n",
    "\n",
    "mean(mu, sigma, gamma, tau=None)\n",
    "    distribution mean.\n",
    "\n",
    "median(mu, sigma, gamma, tau=None)\n",
    "    distribution median.\n",
    "\n",
    "prob(x, mu, sigma, gamma, tau=None)\n",
    "    probability density function (pdf).\n",
    "\n",
    "quantile(pr, mu, sigma, gamma, tau=None)\n",
    "    inverse cumulative distribution function.\n",
    "\n",
    "rvs(mu, sigma, gamma, tau=None, size=1)\n",
    "    generate random variates.\n",
    "\n",
    "stddev(mu, sigma, gamma, tau=None)\n",
    "    distribution standard deviation.\n",
    "\n",
    "variance(mu, sigma, gamma, tau=None)\n",
    "    distribution variance.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "* This module uses only tensorflow.  This module does not use the\n",
    "tensorflow_probability library.\n",
    "\n",
    "* The sinh-arcsinh normal distribution was defined in [1]. A more accessible\n",
    "presentation is given in [2]. \n",
    "\n",
    "* The notation and formulation used in this code was taken from [3], page 143.\n",
    "In the gamlss.dist/CRAN package the distribution is called SHASHo. \n",
    "\n",
    "* There is a typographical error in the presentation of the probability \n",
    "density function on page 143 of [3]. There is an extra \"2\" in the denomenator\n",
    "preceeding the \"sqrt{1 + z^2}\" term.\n",
    "\n",
    "References\n",
    "----------\n",
    "[1] Jones, M. C. & Pewsey, A., Sinh-arcsinh distributions,\n",
    "Biometrika, Oxford University Press, 2009, 96, 761-780.\n",
    "DOI: 10.1093/biomet/asp053.\n",
    "\n",
    "[2] Jones, C. & Pewsey, A., The sinh-arcsinh normal distribution,\n",
    "Significance, Wiley, 2019, 16, 6-7.\n",
    "DOI: 10.1111/j.1740-9713.2019.01245.x.\n",
    "https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2019.01245.x\n",
    "\n",
    "[3] Stasinopoulos, Mikis, et al. (2021), Distributions for Generalized \n",
    "Additive Models for Location Scale and Shape, CRAN Package.\n",
    "https://cran.r-project.org/web/packages/gamlss.dist/gamlss.dist.pdf\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import tensorflow as tf\n",
    "\n",
    "__author__ = \"Randal J. Barnes and Elizabeth A. Barnes\"\n",
    "__date__ = \"14 January 2022\"\n",
    "\n",
    "\n",
    "SQRT_TWO = 1.4142135623730950488016887\n",
    "ONE_OVER_SQRT_TWO = 0.7071067811865475244008444\n",
    "TWO_PI = 6.2831853071795864769252868\n",
    "SQRT_TWO_PI = 2.5066282746310005024157653\n",
    "ONE_OVER_SQRT_TWO_PI = 0.3989422804014326779399461\n",
    "\n",
    "\n",
    "def _jones_pewsey_P(q):\n",
    "    \"\"\"P_q function from page 764 of [1].\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    q : float, array like\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    P_q : array like of same shape as q.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * The formal equation is\n",
    "\n",
    "            jp = 0.25612601391340369863537463 * (\n",
    "                scipy.special.kv((q + 1) / 2, 0.25) +\n",
    "                scipy.special.kv((q - 1) / 2, 0.25)\n",
    "            )\n",
    "\n",
    "        The strange constant 0.25612... is \"sqrt( sqrt(e) / (8*pi) )\" computed\n",
    "        with a high-precision calculator.  The special function\n",
    "\n",
    "            scipy.special.kv\n",
    "\n",
    "        is the Modified Bessel function of the second kind: K(nu, x).\n",
    "\n",
    "    * But, we cannot use the scipy.special.kv function during tensorflow\n",
    "        training.  This code uses a 6th order polynomial approximation in\n",
    "        place of the formal function.\n",
    "\n",
    "    * This approximation is well behaved for 0 <= q <= 10. Since q = 1/tau\n",
    "        or q = 2/tau in our applications, the approximation is well behaved\n",
    "        for 1/10 <= tau < infty.\n",
    "\n",
    "    \"\"\"\n",
    "    # A 6th order polynomial approximation of log(_jones_pewsey_P) for the\n",
    "    # range 0 <= q <= 10.  Over this range, the max |error|/true < 0.0025.\n",
    "    # These coefficients were computed by minimizing the maximum relative\n",
    "    # error, and not by a simple least squares regression.\n",
    "    coeffs = [\n",
    "        9.37541380598926e-06,\n",
    "        -0.000377732651131894,\n",
    "        0.00642826706073389,\n",
    "        -0.061281078712518,\n",
    "        0.390956214318641,\n",
    "        -0.0337884356755193,\n",
    "        0.00248824801827172\n",
    "    ]\n",
    "    return tf.math.exp(tf.math.polyval(coeffs, q))\n",
    "\n",
    "\n",
    "def shash_cdf(x, mu, sigma, gamma, tau=None):\n",
    "    \"\"\"Cumulative distribution function (cdf).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float (batch size x 1) Tensor\n",
    "        The values at which to compute the probability density function.\n",
    "\n",
    "    mu : float (batch size x 1) Tensor\n",
    "        The location parameter. Must be the same shape as x.\n",
    "\n",
    "    sigma : float (batch size x 1) Tensor\n",
    "        The scale parameter. Must be strictly positive. Must be the same\n",
    "        shape as x.\n",
    "\n",
    "    gamma : float (batch size x 1) Tensor\n",
    "        The skewness parameter. Must be the same shape as x.\n",
    "\n",
    "    tau : float (batch size x 1) Tensor or None\n",
    "        The tail-weight parameter. Must be strictly positive. Must be the same\n",
    "        shape as x. If tau is None then the default value of tau=1 is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    F : float (batch size x 1) Tensor.\n",
    "        The computed cumulative probability distribution function (cdf)\n",
    "        evaluated at the values of x.  F has the same shape as x.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * This function uses the tensorflow.math.erf function rather than the\n",
    "    tensorflow_probability normal distribution functions.\n",
    "\n",
    "    \"\"\"\n",
    "    y = (x - mu) / sigma    \n",
    "    \n",
    "    if tau is None:\n",
    "        z = tf.math.sinh(tf.math.asinh(y) - gamma)\n",
    "    else:\n",
    "        z = tf.math.sinh(tau * tf.math.asinh(y) - gamma)\n",
    "\n",
    "    return 0.5 * (1.0 + tf.math.erf(ONE_OVER_SQRT_TWO * z))\n",
    "\n",
    "\n",
    "def shash_log_prob(x, mu, sigma, gamma, tau=None):\n",
    "    \"\"\"Log-probability density function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float (batch size x 1) Tensor\n",
    "        The values at which to compute the probability density function.\n",
    "\n",
    "    mu : float (batch size x 1) Tensor\n",
    "        The location parameter. Must be the same shape as x.\n",
    "\n",
    "    sigma : float (batch size x 1) Tensor\n",
    "        The scale parameter. Must be strictly positive. Must be the same\n",
    "        shape as x.\n",
    "\n",
    "    gamma : float (batch size x 1) Tensor\n",
    "        The skewness parameter. Must be the same shape as x.\n",
    "\n",
    "    tau : float (batch size x 1) Tensor\n",
    "        The tail-weight parameter. Must be strictly positive. Must be the same\n",
    "        shape as x. If tau is None then the default value of tau=1 is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f : float (batch size x 1) Tensor.\n",
    "        The natural logarithm of the computed probability density function\n",
    "        evaluated at the values of x.  f has the same shape as x.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * This function is included merely to emulate the tensorflow_probability\n",
    "    distributions.\n",
    "\n",
    "    \"\"\"\n",
    "    return tf.math.log(shash_prob(x, mu, sigma, gamma, tau))\n",
    "\n",
    "\n",
    "def shash_mean(mu, sigma, gamma, tau=None):\n",
    "    \"\"\"The distribution mean.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    mu : float (batch size x 1) Tensor\n",
    "        The location parameter.\n",
    "\n",
    "    sigma : float (batch size x 1) Tensor\n",
    "        The scale parameter. Must be strictly positive. Must be the same\n",
    "        shape as mu.\n",
    "\n",
    "    gamma : float (batch size x 1) Tensor\n",
    "        The skewness parameter. Must be the same shape as mu.\n",
    "\n",
    "    tau : float (batch size x 1) Tensor\n",
    "        The tail-weight parameter. Must be strictly positive. Must be the same\n",
    "        shape as mu. If tau is None then the default value of tau=1 is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : float (batch size x 1) Tensor.\n",
    "        The computed distribution mean values.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * This equation for evX can be found on page 764 of [1].\n",
    "\n",
    "    \"\"\"\n",
    "    if tau is None:\n",
    "        evX = tf.math.sinh(gamma) * 1.35453080648132  \n",
    "    else:\n",
    "        evX = tf.math.sinh(gamma / tau) * _jones_pewsey_P(1.0 / tau)\n",
    "\n",
    "    return mu + sigma * evX\n",
    "\n",
    "\n",
    "def shash_median(mu, sigma, gamma, tau=None):\n",
    "    \"\"\"The distribution median.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    mu : float (batch size x 1) Tensor\n",
    "        The location parameter.\n",
    "\n",
    "    sigma : float (batch size x 1) Tensor\n",
    "        The scale parameter. Must be strictly positive. Must be the same\n",
    "        shape as mu.\n",
    "\n",
    "    gamma : float (batch size x 1) Tensor\n",
    "        The skewness parameter. Must be the same shape as mu.\n",
    "\n",
    "    tau : float (batch size x 1) Tensor\n",
    "        The tail-weight parameter. Must be strictly positive. Must be the same\n",
    "        shape as mu. If tau is None then the default value of tau=1 is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : float (batch size x 1) Tensor.\n",
    "        The computed distribution mean values.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * This code uses the basic formula:\n",
    "\n",
    "        E(a*X + b) = a*E(X) + b\n",
    "\n",
    "    * The E(X) is computed using the moment equation given on page 764 of [1].\n",
    "\n",
    "    \"\"\"\n",
    "    if tau is None:\n",
    "        return mu + sigma * tf.math.sinh(gamma)\n",
    "    else:\n",
    "        return mu + sigma * tf.math.sinh(gamma / tau)\n",
    "\n",
    "\n",
    "def shash_prob(x, mu, sigma, gamma, tau=None):\n",
    "    \"\"\"Probability density function (pdf).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float (batch size x 1) Tensor\n",
    "        The values at which to compute the probability density function.\n",
    "\n",
    "    mu : float (batch size x 1) Tensor\n",
    "        The location parameter. Must be the same shape as x.\n",
    "\n",
    "    sigma : float (batch size x 1) Tensor\n",
    "        The scale parameter. Must be strictly positive. Must be the same\n",
    "        shape as x.\n",
    "\n",
    "    gamma : float (batch size x 1) Tensor\n",
    "        The skewness parameter. Must be the same shape as x.\n",
    "\n",
    "    tau : float (batch size x 1) Tensor\n",
    "        The tail-weight parameter. Must be strictly positive. Must be the same\n",
    "        shape as x. If tau is None then the default value of tau=1 is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f : float (batch size x 1) Tensor.\n",
    "        The computed probability density function evaluated at the values of x.\n",
    "        f has the same shape as x.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * This code uses the equations on page 143 of [3], and the associated\n",
    "    notation.\n",
    "\n",
    "    \"\"\"\n",
    "    y = (x - mu) / sigma\n",
    "    \n",
    "    if tau is None:\n",
    "        rsqr = tf.math.square(tf.math.sinh(tf.math.asinh(y) - gamma))\n",
    "        return (\n",
    "            ONE_OVER_SQRT_TWO_PI\n",
    "            / sigma\n",
    "            * tf.math.sqrt((1 + rsqr) / (1 + tf.math.square(y)))\n",
    "            * tf.math.exp(-rsqr / 2)\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        rsqr = tf.math.square(tf.math.sinh(tau * tf.math.asinh(y) - gamma))\n",
    "        return (\n",
    "            ONE_OVER_SQRT_TWO_PI\n",
    "            * (tau / sigma)\n",
    "            * tf.math.sqrt((1 + rsqr) / (1 + tf.math.square(y)))\n",
    "            * tf.math.exp(-rsqr / 2)\n",
    "        )\n",
    "\n",
    "\n",
    "def shash_quantile(pr, mu, sigma, gamma, tau=None):\n",
    "    \"\"\"Inverse cumulative distribution function.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    pr : float (batch size x 1) Tensor.\n",
    "        The probabilities at which to compute the values.\n",
    "\n",
    "    mu : float (batch size x 1) Tensor\n",
    "        The location parameter. Must be the same shape as pr.\n",
    "\n",
    "    sigma : float (batch size x 1) Tensor\n",
    "        The scale parameter. Must be strictly positive. Must be the same\n",
    "        shape as pr.\n",
    "\n",
    "    gamma : float (batch size x 1) Tensor\n",
    "        The skewness parameter. Must be the same shape as pr.\n",
    "\n",
    "    tau : float (batch size x 1) Tensor\n",
    "        The tail-weight parameter. Must be strictly positive. Must be the same\n",
    "        shape as pr. If tau is None then the default value of tau=1 is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : float (batch size x 1) Tensor.\n",
    "        The computed values at the specified probabilities. f has the same\n",
    "        shape as pr.\n",
    "\n",
    "    \"\"\"\n",
    "    z = tf.math.ndtri(pr)\n",
    "            \n",
    "    if tau is None:\n",
    "        return mu + sigma * tf.math.sinh(tf.math.asinh(z) + gamma)    \n",
    "    else:\n",
    "        return mu + sigma * tf.math.sinh((tf.math.asinh(z) + gamma) / tau)\n",
    "\n",
    "\n",
    "def shash_rvs(mu, sigma, gamma, tau=None, size=1):\n",
    "    \"\"\"Generate an array of random variates.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    mu : float or double scalar\n",
    "        The location parameter.\n",
    "\n",
    "    sigma : float or double scalar\n",
    "        The scale parameter. Must be strictly positive.\n",
    "\n",
    "    gamma : float or double scalar\n",
    "        The skewness parameter.\n",
    "\n",
    "    tau : float or double scalar, or None\n",
    "        The tail-weight parameter. Must be strictly positive. \n",
    "        If tau is None then the default value of tau=1 is used.\n",
    "\n",
    "    size : int or tuple of ints, default=1.\n",
    "        The number of random variates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : double ndarray of size=size\n",
    "        The generated random variates.\n",
    "\n",
    "    \"\"\"\n",
    "    z = scipy.stats.norm.rvs(size=size)\n",
    "    \n",
    "    if tau is None:\n",
    "        return mu + sigma * np.sinh(np.arcsinh(z) + gamma)\n",
    "    else:\n",
    "        return mu + sigma * np.sinh((np.arcsinh(z) + gamma) / tau)\n",
    "\n",
    "\n",
    "def shash_stddev(mu, sigma, gamma, tau=None):\n",
    "    \"\"\"The distribution standard deviation.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    mu : float (batch size x 1) Tensor\n",
    "        The location parameter.\n",
    "\n",
    "    sigma : float (batch size x 1) Tensor\n",
    "        The scale parameter. Must be strictly positive. Must be the same\n",
    "        shape as mu.\n",
    "\n",
    "    gamma : float (batch size x 1) Tensor\n",
    "        The skewness parameter. Must be the same shape as mu.\n",
    "\n",
    "    tau : float (batch size x 1) Tensor\n",
    "        The tail-weight parameter. Must be strictly positive. Must be the same\n",
    "        shape as mu. If tau is None then the default value of tau=1 is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : float (batch size x 1) Tensor.\n",
    "        The computed distribution standard deviation values.\n",
    "\n",
    "    \"\"\"\n",
    "    return tf.math.sqrt(variance(mu, sigma, gamma, tau))\n",
    "\n",
    "\n",
    "def shash_variance(mu, sigma, gamma, tau=None):\n",
    "    \"\"\"The distribution variance.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    mu : float (batch size x 1) Tensor\n",
    "        The location parameter.\n",
    "\n",
    "    sigma : float (batch size x 1) Tensor\n",
    "        The scale parameter. Must be strictly positive. Must be the same\n",
    "        shape as mu.\n",
    "\n",
    "    gamma : float (batch size x 1) Tensor\n",
    "        The skewness parameter. Must be the same shape as mu.\n",
    "\n",
    "    tau : float (batch size x 1) Tensor\n",
    "        The tail-weight parameter. Must be strictly positive. Must be the same\n",
    "        shape as mu. If tau is None then the default value of tau=1 is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : float (batch size x 1) Tensor.\n",
    "        The computed distribution variance values.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * This code uses two basic formulas:\n",
    "\n",
    "        var(X) = E(X^2) - (E(X))^2\n",
    "        var(a*X + b) = a^2 * var(X)\n",
    "\n",
    "    * The E(X) and E(X^2) are computed using the moment equations given on\n",
    "    page 764 of [1].\n",
    "\n",
    "    \"\"\"\n",
    "    if tau is None:\n",
    "        evX = tf.math.sinh(gamma) * 1.35453080648132\n",
    "        evX2 = (tf.math.cosh(2 * gamma) * 3.0 - 1.0) / 2\n",
    "    else:\n",
    "        evX = tf.math.sinh(gamma / tau) * _jones_pewsey_P(1.0 / tau)\n",
    "        evX2 = (tf.math.cosh(2 * gamma / tau) * _jones_pewsey_P(2.0 / tau) - 1.0) / 2\n",
    "        \n",
    "    return tf.math.square(sigma) * (evX2 - tf.math.square(evX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blLQldL6jxLS"
   },
   "source": [
    "### Build the SHASH tf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1645453442059,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "38SCN_19hmfJ"
   },
   "outputs": [],
   "source": [
    "def make_model(settings, x_train, onehot_train, model_compile=False):\n",
    "    if settings[\"uncertainty_type\"][:5] == \"shash\":   \n",
    "        model = build_shash_model(\n",
    "            x_train,\n",
    "            onehot_train,\n",
    "            hiddens=settings[\"hiddens\"],\n",
    "            output_shape=onehot_train.shape[1],\n",
    "            ridge_penalty=settings[\"ridge_param\"],\n",
    "            act_fun=settings[\"act_fun\"],\n",
    "            dropout_rate=settings[\"dropout_rate\"],\n",
    "            rng_seed=settings[\"rng_seed\"],                    \n",
    "        )\n",
    "\n",
    "        if model_compile == True:        \n",
    "            model.compile(\n",
    "                optimizer=optimizers.SGD(\n",
    "                    learning_rate=settings[\"learning_rate\"],\n",
    "                    momentum=settings[\"momentum\"],\n",
    "                    nesterov=settings[\"nesterov\"],\n",
    "                ),\n",
    "                loss=compute_shash_NLL,\n",
    "                metrics=[\n",
    "                    CustomMAE(name=\"custom_mae\"),\n",
    "                    InterquartileCapture(name=\"interquartile_capture\"),\n",
    "                    SignTest(name=\"sign_test\"),\n",
    "                ],\n",
    "            )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def build_shash_model(\n",
    "    x_train, onehot_train, hiddens, output_shape, ridge_penalty=[0.0,], act_fun=\"relu\", rng_seed=999, dropout_rate=[0.0,],\n",
    "):\n",
    "    \"\"\"Build the fully-connected shash network architecture with\n",
    "    internal scaling.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    x_train : numpy.ndarray\n",
    "        The training split of the x data.\n",
    "        shape = [n_train, n_features].\n",
    "\n",
    "    onehot_train : numpy.ndarray\n",
    "        The training split of the scaled y data is in the first column.\n",
    "        The remaining columns are filled with zeros. The number of columns\n",
    "        equal the number of distribution parameters.\n",
    "        shape = [n_train, n_parameters].\n",
    "\n",
    "    hiddens : list (integers)\n",
    "        Numeric list containing the number of neurons for each layer.\n",
    "\n",
    "    output_shape : integer {2, 3, 4}\n",
    "        The number of distribution output parameters to be learned.\n",
    "\n",
    "    ridge_penalty : float, default=0.0\n",
    "        The L2 regularization penalty for the first layer.\n",
    "\n",
    "    act_fun : function, default=\"relu\"\n",
    "        The activation function to use on the deep hidden layers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : tensorflow.keras.models.Model\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * The number of output units is determined by the output_shape argument.\n",
    "        If output_shape is:\n",
    "        2 -> output_layer = [mu_unit, sigma_unit]\n",
    "        3 -> output_layer = [mu_unit, sigma_unit, gamma_unit]\n",
    "        4 -> output_layer = [mu_unit, sigma_unit, gamma_unit, tau_unit]\n",
    "\n",
    "    * Unlike most of EAB's models, the features are normalized within the\n",
    "        network.  That is, the x_train, y_train, ... y_test should not be\n",
    "        externally normalized or scaled.\n",
    "\n",
    "    * In essence, the network is learning the shash parameters for the\n",
    "        normalized y values. Say mu_z and sigma_z where\n",
    "\n",
    "            z = (y - y_avg)/y_std\n",
    "\n",
    "        The mu_unit and sigma_unit layers rescale the learned mu_z\n",
    "        and sigma_z parameters back to the dimensions of the y values.\n",
    "        Specifically,\n",
    "\n",
    "            mu_y    = y_std * mu_z + y_avg\n",
    "            sigma_y = y_std * sigma_z\n",
    "\n",
    "        However, since the model works with log(sigma) we must use\n",
    "\n",
    "            log(sigma_y) = log(y_std * sigma_z) = log(y_std) + log(sigma_z)\n",
    "\n",
    "    * Note the gamma and tau parameters of the shash distribution are\n",
    "        dimensionless by definition. So we do not need to rescale gamma\n",
    "        and tau.\n",
    "\n",
    "    \"\"\"\n",
    "    # set inputs\n",
    "    if len(hiddens) != len(ridge_penalty):\n",
    "        ridge_penalty = np.ones(np.shape(hiddens))*ridge_penalty\n",
    "    if len(hiddens) != len(dropout_rate):\n",
    "        dropout_rate = np.ones((len(hiddens)+1,))*dropout_rate\n",
    "    \n",
    "    # The avg and std for feature normalization are computed from x_train.\n",
    "    # Using the .adapt method, these are set once and do not change, but\n",
    "    # the constants travel with the model.\n",
    "    inputs = tf.keras.Input(shape=x_train.shape[1:])\n",
    "\n",
    "    normalizer = tf.keras.layers.Normalization()\n",
    "    normalizer.adapt(x_train)\n",
    "    x = normalizer(inputs)\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(\n",
    "        rate=dropout_rate[0],\n",
    "        seed=rng_seed,            \n",
    "    )(x)        \n",
    "\n",
    "    # linear network only\n",
    "    if hiddens[0] == 0:\n",
    "        x = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=\"linear\",\n",
    "            use_bias=True,\n",
    "            kernel_regularizer=regularizers.l1_l2(l1=0.00, l2=ridge_penalty[0]),\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed+0),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed+0),\n",
    "        )(x)\n",
    "    else:\n",
    "        # Initialize the first hidden layer.\n",
    "        x = tf.keras.layers.Dense(\n",
    "            units=hiddens[0],\n",
    "            activation=act_fun,\n",
    "            use_bias=True,\n",
    "            kernel_regularizer=regularizers.l1_l2(l1=0.00, l2=ridge_penalty[0]),\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed+0),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed+0),\n",
    "        )(x)\n",
    "\n",
    "        # Initialize the subsequent hidden layers.\n",
    "        for ilayer, layer_size in enumerate(hiddens[1:]):\n",
    "            \n",
    "            x = tf.keras.layers.Dropout(\n",
    "                rate=dropout_rate[ilayer+1],\n",
    "                seed=rng_seed,            \n",
    "            )(x)            \n",
    "            \n",
    "            x = tf.keras.layers.Dense(\n",
    "                units=layer_size,\n",
    "                activation=act_fun,\n",
    "                use_bias=True,\n",
    "                kernel_regularizer=regularizers.l1_l2(l1=0.00, l2=ridge_penalty[ilayer+1]),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed+ilayer+1),\n",
    "                kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed+ilayer+1),\n",
    "            )(x)\n",
    "            \n",
    "    # final dropout prior to output layer\n",
    "    x = tf.keras.layers.Dropout(\n",
    "        rate=dropout_rate[-1],\n",
    "        seed=rng_seed,            \n",
    "    )(x) \n",
    "            \n",
    "    # Compute the mean and standard deviation of the y_train data to rescale\n",
    "    # the mu and sigma parameters.\n",
    "    y_avg = np.mean(onehot_train[:, 0])\n",
    "    y_std = np.std(onehot_train[:, 0])\n",
    "\n",
    "    # mu_unit.  The network predicts the scaled mu_z, then the resclaing\n",
    "    # layer scales it up to mu_y.\n",
    "    mu_z_unit = tf.keras.layers.Dense(\n",
    "        units=1,\n",
    "        activation=\"linear\",\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed+100),\n",
    "        kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed+100),\n",
    "        name=\"mu_z_unit\",\n",
    "    )(x)\n",
    "\n",
    "    mu_unit = tf.keras.layers.Rescaling(\n",
    "        scale=y_std,\n",
    "        offset=y_avg,\n",
    "        name=\"mu_unit\",\n",
    "    )(mu_z_unit)\n",
    "\n",
    "    # sigma_unit. The network predicts the log of the scaled sigma_z, then\n",
    "    # the resclaing layer scales it up to log of sigma y, and the custom\n",
    "    # Exponentiate layer converts it to sigma_y.\n",
    "    log_sigma_z_unit = tf.keras.layers.Dense(\n",
    "        units=1,\n",
    "        activation=\"linear\",\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),\n",
    "        kernel_initializer=tf.keras.initializers.Zeros(),\n",
    "        name=\"log_sigma_z_unit\",\n",
    "    )(x)\n",
    "\n",
    "    log_sigma_unit = tf.keras.layers.Rescaling(\n",
    "        scale=1.0,\n",
    "        offset=np.log(y_std),\n",
    "        name=\"log_sigma_unit\",\n",
    "    )(log_sigma_z_unit)\n",
    "\n",
    "    sigma_unit = Exponentiate(\n",
    "        name=\"sigma_unit\",\n",
    "    )(log_sigma_unit)\n",
    "\n",
    "    # Add gamma and tau units if requested.\n",
    "    if output_shape == 2:\n",
    "        output_layer = tf.keras.layers.concatenate([mu_unit, sigma_unit], axis=1)\n",
    "\n",
    "    else:\n",
    "        # gamma_unit. The network predicts the gamma directly.\n",
    "        gamma_unit = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=\"linear\",\n",
    "            use_bias=True,\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            kernel_initializer=tf.keras.initializers.Zeros(),\n",
    "            name=\"gamma_unit\",\n",
    "        )(x)\n",
    "\n",
    "        if output_shape == 3:\n",
    "            output_layer = tf.keras.layers.concatenate(\n",
    "                [mu_unit, sigma_unit, gamma_unit], axis=1\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # tau_unit. The network predicts the log of the tau, then\n",
    "            # the custom Exponentiate layer converts it to tau.\n",
    "            log_tau_unit = tf.keras.layers.Dense(\n",
    "                units=1,\n",
    "                activation=\"linear\",\n",
    "                use_bias=True,\n",
    "                bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                kernel_initializer=tf.keras.initializers.Zeros(),\n",
    "                name=\"log_tau_unit\",\n",
    "            )(x)\n",
    "\n",
    "            tau_unit = Exponentiate(\n",
    "                name=\"tau_unit\",\n",
    "            )(log_tau_unit)\n",
    "\n",
    "            if output_shape == 4:\n",
    "                output_layer = tf.keras.layers.concatenate(\n",
    "                    [mu_unit, sigma_unit, gamma_unit, tau_unit], axis=1\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=output_layer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NLEo8eFhmfQ"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw54ou9qhmfN"
   },
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1645453442061,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "rgd-JMzmhmfP",
    "outputId": "69164166-053f-4793-917e-4368eda50d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is loaded\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "if IN_COLAB:\n",
    "    !pip install wget\n",
    "    import wget\n",
    "    filename = wget.download(\"https://raw.githubusercontent.com/eabarnes1010/course_ml_ats/main/data/EPCP72_data.pickle\")        \n",
    "else:\n",
    "    filename = '../data/EPCP72_data.pickle'\n",
    "\n",
    "with open(filename,'rb') as f:\n",
    "     data = pickle.load(f)\n",
    "        \n",
    "(   data_summary,\n",
    "    x_train,\n",
    "    onehot_train,\n",
    "    x_val,\n",
    "    onehot_val,\n",
    "    x_test,\n",
    "    onehot_test,    \n",
    "    df_train,\n",
    "    df_val,\n",
    "    df_test,\n",
    ") = data\n",
    "\n",
    "print('data is loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZOJlzaIqe4p"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1645453442061,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "2gzAL-_5hmfQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onehot_train.shape = (1081, 4)\n",
      "onehot_val.shape = (256, 4)\n",
      "onehot_test.shape = (313, 4)\n"
     ]
    }
   ],
   "source": [
    "# ------------ MODIFY ------------\n",
    "# many of the parameters below are things that you can easily modify\n",
    "\n",
    "settings = {\n",
    "        \"filename\": \"nnfit_vlist_intensity_and_track_extended.dat\",\n",
    "        \"uncertainty_type\": 'shash4',  # OPTIONS: \"shash2\", \"shash4\"  \n",
    "        \"leadtime\": 72,\n",
    "        \"basin\": \"EP|CP\",\n",
    "        \"target\": \"intensity\",\n",
    "        \"undersample\": False,\n",
    "        \"hiddens\": [15, 10],\n",
    "        \"dropout_rate\": [0.,0.,0.],\n",
    "        \"ridge_param\": [0.0,0.0],            \n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"momentum\": 0.9,\n",
    "        \"nesterov\": True,\n",
    "        \"batch_size\": 64,\n",
    "        \"rng_seed\": 888,\n",
    "        \"act_fun\": \"relu\",\n",
    "        \"n_epochs\": 25_000,\n",
    "        \"patience\": 100,\n",
    "        \"test_condition\": (2018,),\n",
    "        \"val_condition\": \"random\",\n",
    "        \"n_val\": 256,\n",
    "        \"n_train\": \"max\",               \n",
    "    }\n",
    "\n",
    "if settings[\"uncertainty_type\"]==\"shash2\":\n",
    "    onehot_train = onehot_train[:,0:2]\n",
    "    onehot_val = onehot_val[:,0:2]\n",
    "    onehot_test = onehot_test[:,0:2]    \n",
    "elif settings[\"uncertainty_type\"]==\"shash3\":    \n",
    "    print(\"SHASH3 is how the data was loaded. Changing nothing.\")\n",
    "          \n",
    "elif settings[\"uncertainty_type\"]==\"shash4\":       \n",
    "    onehot_train = np.append(onehot_train,onehot_train[:,1:2],axis=1)\n",
    "    onehot_val = np.append(onehot_val,onehot_val[:,1:2],axis=1)\n",
    "    onehot_test = np.append(onehot_test,onehot_test[:,1:2],axis=1)                \n",
    "else:\n",
    "    raise NotImplementedError(\"No such uncertainty_type implemented here.\")\n",
    "    \n",
    "print(\"onehot_train.shape = \" + str(onehot_train.shape))\n",
    "print(\"onehot_val.shape = \" + str(onehot_val.shape))\n",
    "print(\"onehot_test.shape = \" + str(onehot_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 183410,
     "status": "ok",
     "timestamp": 1645453625392,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "HBQYWNMVhmfR",
    "outputId": "d421f525-318a-4a6d-c94c-57bce849f4fa"
   },
   "outputs": [],
   "source": [
    "# define the callbacks\n",
    "earlystoping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=settings[\"patience\"],\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "training_callback = TrainingInstrumentation(\n",
    "    x_train,\n",
    "    onehot_train,\n",
    "    interval=50,\n",
    ")\n",
    "\n",
    "callbacks = [earlystoping_callback, \n",
    "            #  training_callback,  # uncomment if you want to see the training stats updated\n",
    "            ]\n",
    "\n",
    "# set network seed and train the model\n",
    "NETWORK_SEED_LIST = [settings[\"rng_seed\"]]\n",
    "\n",
    "for network_seed in NETWORK_SEED_LIST:\n",
    "    tf.random.set_seed(network_seed)  # This sets the global random seed.\n",
    "\n",
    "    # Make, compile, and train the model\n",
    "    tf.keras.backend.clear_session()            \n",
    "    model = make_model(\n",
    "        settings,\n",
    "        x_train,\n",
    "        onehot_train,\n",
    "        model_compile=True,\n",
    "    )   \n",
    "    model.summary()\n",
    "\n",
    "    # train the network\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        onehot_train,\n",
    "        validation_data=(x_val, onehot_val),\n",
    "        batch_size=settings[\"batch_size\"],\n",
    "        epochs=settings[\"n_epochs\"],\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    stop_time = time.time()\n",
    "\n",
    "    # Display the results, and save the model rum.\n",
    "    best_epoch = np.argmin(history.history[\"val_loss\"])\n",
    "    fit_summary = {\n",
    "        \"network_seed\": network_seed,\n",
    "        \"elapsed_time\": stop_time - start_time,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"loss_train\": history.history[\"loss\"][best_epoch],\n",
    "        \"loss_valid\": history.history[\"val_loss\"][best_epoch],\n",
    "    }\n",
    "    pprint.pprint(fit_summary, width=80)\n",
    "    plot_history(history, 'class example')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6PwLsqyhmfU"
   },
   "source": [
    "## Plot the predicted distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j09ZZaZmpYhm",
    "tags": []
   },
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1645453625636,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "qyTMi2-No9iC"
   },
   "outputs": [],
   "source": [
    "clr_shash = 'teal'\n",
    "clr_bnn   = 'orange'\n",
    "clr_truth = 'dimgray'\n",
    "\n",
    "### for white background...\n",
    "plt.rc('text',usetex=False)\n",
    "plt.rc('font',**{'family':'sans-serif','sans-serif':['Avant Garde']}) \n",
    "plt.rc('savefig',facecolor='white')\n",
    "plt.rc('axes',facecolor='white')\n",
    "plt.rc('axes',labelcolor='dimgrey')\n",
    "plt.rc('axes',labelcolor='dimgrey')\n",
    "plt.rc('xtick',color='dimgrey')\n",
    "plt.rc('ytick',color='dimgrey')\n",
    "\n",
    "\n",
    "def adjust_spines(ax, spines):\n",
    "    for loc, spine in ax.spines.items():\n",
    "        if loc in spines:\n",
    "            spine.set_position(('outward', 5))\n",
    "        else:\n",
    "            spine.set_color('none')  \n",
    "    if 'left' in spines:\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "    else:\n",
    "        ax.yaxis.set_ticks([])\n",
    "    if 'bottom' in spines:\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "    else:\n",
    "            ax.xaxis.set_ticks([])  \n",
    "\n",
    "def plot_sample(ax, onehot_val, shash_incs, shash_cpd, sample=130):\n",
    "    plt.sca(ax)  \n",
    "\n",
    "    if(shash_cpd.shape[0]<sample):\n",
    "        sample = shash_cpd.shape[0]-1\n",
    "    \n",
    "    bins = np.arange(np.min(shash_incs),np.max(shash_incs)+2,2)\n",
    "\n",
    "    # results for SHASH\n",
    "    plt.plot(shash_incs,\n",
    "             shash_cpd[sample,:],\n",
    "             color=clr_shash,\n",
    "             linewidth=4,\n",
    "             label='SHASH',\n",
    "            )\n",
    "\n",
    "    # truth\n",
    "    plt.axvline(x=onehot_val[sample,0],color=clr_truth,linestyle='--', label='Actual / Label')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.gca()\n",
    "    xticks = ax.get_xticks()\n",
    "    yticks = np.around(ax.get_yticks(),3)\n",
    "    plt.xticks(xticks.astype(int),xticks.astype(int))\n",
    "    plt.yticks(yticks,yticks)\n",
    "\n",
    "    plt.title('Sample ' + str(sample),fontsize=16)\n",
    "    plt.xlabel('predicted deviation from consensus (knots)')\n",
    "    plt.ylabel('probability density function')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg-gkqUXpVM5"
   },
   "source": [
    "### Compute the predicted distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZITrCh_hhmfU"
   },
   "source": [
    "First we need to evaluate and create the PDFs for the predicted parameters - otherwise, we would just be plotting the predicted parameters of the SHASH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14473,
     "status": "ok",
     "timestamp": 1645453640036,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "pz28_qQ6hmfV",
    "outputId": "116ab0e1-61b6-49fd-ec1b-9914c4f00484"
   },
   "outputs": [],
   "source": [
    "x_eval = x_test\n",
    "onehot_eval = onehot_test\n",
    "    \n",
    "shash_incs = np.arange(-160,161,1)\n",
    "shash_cpd = np.zeros((np.shape(x_eval)[0],len(shash_incs)))\n",
    "shash_med = np.zeros((np.shape(x_eval)[0],))\n",
    "\n",
    "# loop through samples for shash calculation and get PDF for each sample\n",
    "for j in tqdm(range(0,np.shape(shash_cpd)[0])):\n",
    "    mu_pred, sigma_pred, gamma_pred, tau_pred = params( x_eval[np.newaxis,j], model )\n",
    "    shash_cpd[j,:] = shash_prob(shash_incs, mu_pred, sigma_pred, gamma_pred, tau_pred)    \n",
    "    shash_med[j]   = shash_median(mu_pred,sigma_pred,gamma_pred,tau_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBXXn5a3pd58"
   },
   "source": [
    "### Make plots of multiple samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3158,
     "status": "ok",
     "timestamp": 1645453643112,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "G-gt-dD9hmfV",
    "outputId": "df17de94-4fa9-42b9-8cf8-503deb0ff1a1"
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(999)\n",
    "f, axs = plt.subplots(5, 3, figsize=(15*.85,20*.85))\n",
    "axs = axs.flatten()\n",
    "random_samples = rng.choice(np.arange(0,onehot_eval.shape[0]),len(axs),replace=False)\n",
    "\n",
    "for isample, sample in enumerate(random_samples):\n",
    "    ax = axs[isample]\n",
    "    plot_sample(ax, onehot_eval, shash_incs, shash_cpd, sample=sample)\n",
    "\n",
    "    ax.set_xticks(ticks=np.arange(-200,200,20),minor=False)    \n",
    "    ax.set_xticklabels(np.arange(-200,200,20))\n",
    "    ax.set_xlim(-75,75)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_ylim(-0.0005,None)\n",
    "plt.tight_layout()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "w2Vz-7EXrFcF",
    "Z9HL7n3Sjmaf",
    "1olTfiOhqKPg",
    "cmrETbrpkt6e",
    "3V0tnCQYkcrS",
    "blLQldL6jxLS",
    "Iw54ou9qhmfN",
    "cZOJlzaIqe4p",
    "j09ZZaZmpYhm",
    "Zg-gkqUXpVM5"
   ],
   "name": "ann_uq_hurricanes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
