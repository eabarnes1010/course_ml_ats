{"cells":[{"cell_type":"markdown","metadata":{"id":"9QQ0q47JvgwX"},"source":["# Decision Tree Example using atmospheric data from Christman Field\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eabarnes1010/course_ml_ats/blob/main/code/tree_classifier_christman_hourly_competition.ipynb)\n","\n","* Iris example adapted from: https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n","* Further modified by: Aaron Hill and Wei-Ting Hsiao (Dept. of Atmospheric Science, Colorado State University), January 2020\n","* Further adapted by: Prof. Elizabeth Barnes for ATS 780A7 Spring 2022 at Colorado State University"]},{"cell_type":"markdown","metadata":{"id":"7V55BtYqA52O"},"source":["Lets import some libraries we will need throughout this tutorial:\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1644861796152,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"},"user_tz":420},"id":"YQUUpem1c3tt","outputId":"9a6036a1-b7e0-4e7e-d77e-0ee97516cc56"},"outputs":[{"output_type":"stream","name":"stdout","text":["IN_COLAB = True\n"]}],"source":["try:\n","    import google.colab\n","    IN_COLAB = True\n","except:\n","    IN_COLAB = False\n","print('IN_COLAB = ' + str(IN_COLAB))"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TLuuXcBoA5TN","executionInfo":{"status":"ok","timestamp":1644861797636,"user_tz":420,"elapsed":1512,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"}}},"outputs":[],"source":["import sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import pandas as pd\n","import datetime\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import export_graphviz\n","from sklearn.inspection import permutation_importance\n","import pydot\n","import matplotlib.pyplot as plt\n","# %matplotlib inline"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1644861797637,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"},"user_tz":420},"id":"tO0J_hp2NGIn","outputId":"a6b5c8ed-39fd-4935-d81b-cd4b00ada36b"},"outputs":[{"output_type":"stream","name":"stdout","text":["python version = 3.7.12 (default, Jan 15 2022, 18:48:18) \n","[GCC 7.5.0]\n","numpy version = 1.19.5\n","scikit-learn version = 1.0.2\n"]}],"source":["print(f\"python version = {sys.version}\")\n","print(f\"numpy version = {np.__version__}\")\n","print(f\"scikit-learn version = {sklearn.__version__}\")  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMfu0LxrVBuL"},"outputs":[],"source":["if(IN_COLAB==True):\n","    try:\n","        from google.colab import drive\n","        drive.mount('/content/drive', force_remount=True)\n","        local_path = '/content/drive/My Drive/Colab Notebooks/'\n","    except:\n","        local_path = './'\n","else:\n","    local_path = 'figures/'"]},{"cell_type":"markdown","metadata":{"id":"pq0ubEIlhJ4Y"},"source":["# 1. Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"8TV70rZ43-A6"},"source":["### 1.1 Data overview"]},{"cell_type":"markdown","metadata":{"id":"e_adDXBpYzKN"},"source":["We have stored a .csv file on a CSU drive, accessible via URL. This will be the basis for our tutorial. This file contains Fort Collins weather data from 2020, and we will use these data to predict the high temperature for a given day with a random forest regression model.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PlulN6WnZ_K8"},"outputs":[],"source":["# Read in data from url\n","url = \"https://raw.githubusercontent.com/eabarnes1010/course_ml_ats/main/data/fccwx_data_2020.csv\"\n","data = pd.read_csv(url,parse_dates=[\"Date\"],infer_datetime_format=True)\n","data['dayofyear'] = data['Date'].dt.dayofyear\n","data.reindex(index=data.index[::-1])"]},{"cell_type":"markdown","metadata":{"id":"OqVhzCoFB2wx"},"source":["Lets look at our data to see what we are working with"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibJ9lUMQB8le"},"outputs":[],"source":["# Display first 5 rows\n","print('The shape of our features is:', data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZSlkf1u2ZyU"},"outputs":[],"source":["# A handy tool in pandas: descriptive statistics for each column\n","data.describe()"]},{"cell_type":"markdown","metadata":{"id":"6vLWjiNs4OB7"},"source":["### 1.2 Targets and features"]},{"cell_type":"markdown","metadata":{"id":"WsaFBLnKDPbK"},"source":["The pandas table is handy for a quick glance, but we need to organize some numpy arrays that separately contain our features and labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlt6GqYwhgnV"},"outputs":[],"source":["THRESHOLD_TEMP = 10   # default = 10\n","TARGET_VAR = 'Temp [C]'\n","\n","# Labels are the values we want to predict\n","labels = np.zeros((np.shape(data[TARGET_VAR])))\n","i = np.where(data[TARGET_VAR] < THRESHOLD_TEMP)[0]  # 10C\n","labels[i] = 1\n","i = np.where(data[TARGET_VAR] < -THRESHOLD_TEMP/2)[0]  # -5C\n","labels[i] = 2\n","\n","# Remove the labels from the features\n","# axis 1 refers to the columns\n","features = data.drop(TARGET_VAR, axis = 1)\n","\n","# Also remove DewPt and Date\n","features = features.drop('DewPt [C]', axis = 1)   # comment out if you want the prediction task to be easy\n","features = features.drop('Date', axis = 1)\n","\n","# Saving feature names for later use\n","feature_list = list(features.columns)\n","\n","# Convert to numpy array\n","features = np.array(features)"]},{"cell_type":"markdown","metadata":{"id":"8pRuZk-AhjAP"},"source":["### 1.3 Splitting training and testing datasets"]},{"cell_type":"markdown","metadata":{"id":"95YMbR1YDcqT"},"source":["Assuming we have no feature data available from 2019 we could use to test our trained models against, we will want to split up our dataset into training and testing portions. A standard proportion is 3/4 for training, 1/4 for testing, although this is somewhat arbitrary here. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggg3rkEphk2N"},"outputs":[],"source":["# Split the data into training and testing sets\n","\n","# Tunable Parameter: Describes the proportion of the dataset we want to use for testing. 1 - split_size is used for training. \n","split_size = 0.25\n","\n","# PARAMETERS:\n","#     test_size: fraction of testing/validation datasets\n","#     random_state: random parameter\n","train_features, val_features, train_labels, val_labels = train_test_split(features, labels, test_size = split_size, random_state = 42)"]},{"cell_type":"markdown","metadata":{"id":"SfMNcTlSD7sn"},"source":["Lets quickly check the size of our training and testing arrays are what we expect (and we didn't do something wrong)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHsogVIIhm5X"},"outputs":[],"source":["print('Training Features Shape:', train_features.shape)\n","print('Training Labels Shape:', train_labels.shape)\n","print('Validation Features Shape:', val_features.shape)\n","print('Validation Labels Shape:', val_labels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a_UYYkShVBuT"},"outputs":[],"source":["plt.plot(features[:,-1],np.array(data[TARGET_VAR]),'.')\n","plt.axhline(y=THRESHOLD_TEMP,linestyle='--',color='k')\n","plt.axhline(y=-THRESHOLD_TEMP/2,linestyle='--',color='k')\n","plt.xlabel('day of year')\n","plt.ylabel(TARGET_VAR)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"PmnUiJhK4pQm"},"source":["# 2. Creating a decision tree"]},{"cell_type":"markdown","metadata":{"id":"MoqMJlGiVBuT"},"source":["### Train the model and visualize it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJx8TDwJhvqm"},"outputs":[],"source":["# SINGLE DECISION TREE\n","\n","# Tunable Parameters for Model\n","# tree_depth = 2 \n","# node_split = 2       # minimum number of training samples needed to split a node\n","# leaf_samples = 1     # minimum number of training samples required to make a leaf node\n","# RAND_STATE = 42\n","\n","# tree_clf = DecisionTreeClassifier(max_depth=tree_depth, \n","#                                   min_samples_split=node_split,\n","#                                   min_samples_leaf=leaf_samples,\n","#                                   random_state=RAND_STATE,\n","#                                   criterion='gini',  #can also set to 'entropy'\n","#                                  )\n","# tree_clf.fit(train_features,train_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRLNpsHgVBuU"},"outputs":[],"source":["# RANDOM FOREST\n","# DO NO UCOMMENT THIS BLOCK\n","\n","\n","tree_depth = 2 \n","tree_depth = 15\n","node_split = 2       # minimum number of training samples needed to split a node\n","node_split = 10\n","leaf_samples = 2     # minimum number of training samples required to make a leaf node\n","leaf_samples = 20\n","RAND_STATE = 42\n","\n","number_of_trees = 1\n","\n","tree_clf = RandomForestClassifier(n_estimators = number_of_trees, \n","                           random_state = RAND_STATE,\n","                           min_samples_split = node_split,\n","                           min_samples_leaf = leaf_samples,\n","                           criterion = 'gini',\n","                           max_depth = tree_depth)\n","tree_clf.fit(train_features,train_labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5rjXWUoVBuU"},"outputs":[],"source":["from graphviz import Source\n","fig_savename = 'tree_classifier_christman'\n","\n","if(len(np.shape(tree_clf))!=0):\n","    tree_to_viz = tree_clf[0]\n","else:\n","    tree_to_viz = tree_clf\n","export_graphviz(tree_to_viz,\n","                out_file=local_path + fig_savename+'.dot',\n","                filled=True,\n","                proportion=False,\n","                leaves_parallel=False,\n","                class_names=('above ' + str(THRESHOLD_TEMP) + 'C', 'below ' + str(THRESHOLD_TEMP) + 'C', 'really below ' + str(THRESHOLD_TEMP) + 'C'),\n","                feature_names=feature_list)\n","Source.from_file(local_path + fig_savename+'.dot')"]},{"cell_type":"markdown","metadata":{"id":"S0_gSkHrhySe"},"source":["### Make predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nj7-Y8xShz3C"},"outputs":[],"source":["y_pred_train = tree_clf.predict(train_features)\n","y_pred_val = tree_clf.predict(val_features)\n","y_pred_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dNfRvrsnVBuV"},"outputs":[],"source":["tree_clf.predict_proba(val_features)[:5]"]},{"cell_type":"markdown","metadata":{"id":"zEEvFIRYiRG7"},"source":["### Visualization of predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTXwhWYQVBuV"},"outputs":[],"source":["val_features.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0TKjxzDVBuV"},"outputs":[],"source":["plt.plot(features[:,-1],np.array(data[TARGET_VAR]),'.')\n","plt.plot(train_features[:,-1],(-y_pred_train*15)+15,'.r')\n","plt.axhline(y=THRESHOLD_TEMP,linestyle='--',color='k')\n","plt.axhline(y=-THRESHOLD_TEMP/2,linestyle='--',color='k')\n","plt.xlabel('day of year')\n","plt.ylabel(TARGET_VAR)\n","plt.show()\n","\n","plt.plot(features[:,-1],np.array(data[TARGET_VAR]),'.')\n","plt.plot(val_features[:,-1],(-y_pred_val*15)+15,'.r')\n","plt.axhline(y=THRESHOLD_TEMP,linestyle='--',color='k')\n","plt.axhline(y=-THRESHOLD_TEMP/2,linestyle='--',color='k')\n","plt.xlabel('day of year')\n","plt.ylabel(TARGET_VAR)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ykoWXlp6VBuW"},"source":["### Evaluate the classification predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XFOa431VBuW"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgqujcGFVBuW"},"outputs":[],"source":["# y_probs = tree_clf.predict_proba(val_features)\n","# y_scores = y_probs[:,1]\n","# fpr_tree, tpr_tree, thresholds_tree = roc_curve(val_labels,y_scores)\n","# auc_tree = roc_auc_score(val_labels,y_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHGCxJX7VBuW"},"outputs":[],"source":["# def plot_roc_curve(fpr, tpr, label=None):\n","#     plt.plot(fpr, tpr, linewidth=2, label=label)\n","#     plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n","#     plt.axis([0, 1.01, 0, 1.01])                                    # Not shown in the book\n","#     plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n","#     plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n","#     plt.grid(True)                                            # Not shown\n","    \n","# plt.figure(figsize=(8, 6))\n","# plot_roc_curve(fpr_tree, tpr_tree, \"Decision Tree\")\n","# plt.title('AUC = ' + str(auc_tree))\n","# plt.grid(True)\n","# plt.legend(loc=\"lower right\", fontsize=16)\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVpaL66AVBuW"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred_train = tree_clf.predict(train_features)\n","print('training confusion matrix')\n","print(confusion_matrix(train_labels, y_pred_train))\n","ConfusionMatrixDisplay.from_predictions(train_labels, y_pred_train,normalize='true')\n","plt.title('Training Data')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GxbWwwlVBuX"},"outputs":[],"source":["y_pred_val = tree_clf.predict(val_features)\n","print('validation confusion matrix')\n","print(confusion_matrix(val_labels, y_pred_val))\n","\n","ConfusionMatrixDisplay.from_predictions(val_labels, y_pred_val,normalize='true')\n","plt.title('Validation Data')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soca30RPVBuX"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","print(accuracy_score(train_labels, y_pred_train))\n","print(accuracy_score(val_labels, y_pred_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKgMVklCVBuX"},"outputs":[],"source":["# from sklearn.metrics import precision_score, recall_score\n","\n","# print(precision_score(train_labels, y_pred_train))\n","# print(precision_score(val_labels, y_pred_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJMDdz6gVBuX"},"outputs":[],"source":["# print(recall_score(train_labels, y_pred_train))\n","# print(recall_score(val_labels, y_pred_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QwWU3kG-VBuX"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","print('Macro F1-Score')\n","print(f1_score(train_labels, y_pred_train, average='macro'))\n","print(f1_score(val_labels, y_pred_val, average='macro'))\n","\n","print('')\n","\n","print('Weighted F1-Score')\n","print(f1_score(train_labels, y_pred_train, average='weighted'))\n","print(f1_score(val_labels, y_pred_val, average='weighted'))"]},{"cell_type":"markdown","source":["## Iterate through possibilities and plot"],"metadata":{"id":"aSgOGuptSDSU"}},{"cell_type":"code","source":["import itertools\n","tree_depths = (5, 10)\n","node_splits = (2, 5, 10)\n","leaf_samples = (2, 5, 10)\n","for tree_depth, node_split, leaf_sample in itertools.product(\n","    tree_depths, node_splits, leaf_samples\n","):\n","  # tree_depth = 2 \n","  # tree_depth = 15\n","  # node_split = 2       # minimum number of training samples needed to split a node\n","  # node_split = 10\n","  # leaf_samples = 2     # minimum number of training samples required to make a leaf node\n","  # leaf_samples = 20\n","  # RAND_STATE = 42\n","  print('')\n","  label = f'Depth: {tree_depth} Split: {node_split} Leaf: {leaf_sample}'\n","  print(label)\n","  number_of_trees = 1\n","\n","  tree_clf = RandomForestClassifier(n_estimators = number_of_trees, \n","                            random_state = RAND_STATE,\n","                            min_samples_split = node_split,\n","                            min_samples_leaf = leaf_sample,\n","                            criterion = 'gini',\n","                            max_depth = tree_depth)\n","  tree_clf.fit(train_features,train_labels)\n","\n","  y_pred_train = tree_clf.predict(train_features)\n","  y_pred_val = tree_clf.predict(val_features)\n","  print('Counts:', (y_pred_val == 0).sum(), (y_pred_val == 1).sum(), (y_pred_val == 2).sum())\n","\n","  fig, axs = plt.subplots(ncols=3, figsize=(12, 4))\n","  fig.suptitle(label)\n","  ax = axs[0]\n","  ax.plot(features[:,-1],np.array(data[TARGET_VAR]),'.')\n","  ax.plot(train_features[:,-1],(-y_pred_train*15)+15,'.r')\n","  ax.axhline(y=THRESHOLD_TEMP,linestyle='--',color='k')\n","  ax.axhline(y=-THRESHOLD_TEMP/2,linestyle='--',color='k')\n","  ax.set_xlabel('day of year')\n","  ax.set_ylabel(TARGET_VAR)\n","\n","  ax = axs[1]\n","  ax.plot(features[:,-1],np.array(data[TARGET_VAR]),'.')\n","  ax.plot(val_features[:,-1],(-y_pred_val*15)+15,'.r')\n","  ax.axhline(y=THRESHOLD_TEMP,linestyle='--',color='k')\n","  ax.axhline(y=-THRESHOLD_TEMP/2,linestyle='--',color='k')\n","  ax.set_xlabel('day of year')\n","  ax.set_ylabel(TARGET_VAR)\n","\n","  ax = axs[2]\n","  from sklearn.metrics import confusion_matrix\n","  from sklearn.metrics import ConfusionMatrixDisplay\n","  y_pred_train = tree_clf.predict(train_features)\n","  # print(confusion_matrix(train_labels, y_pred_train))\n","  ConfusionMatrixDisplay.from_predictions(train_labels, y_pred_train,normalize='true', ax=ax)\n","  ax.set_title('Training Data')\n","\n","  from sklearn.metrics import f1_score\n","\n","  print('Macro F1-Score')\n","  print(f1_score(train_labels, y_pred_train, average='macro'))\n","  print(f1_score(val_labels, y_pred_val, average='macro'))\n","  print('Weighted F1-Score')\n","  print(f1_score(train_labels, y_pred_train, average='weighted'))\n","  print(f1_score(val_labels, y_pred_val, average='weighted'))"],"metadata":{"id":"DRyX5XFxSCoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rn15156eVBuY"},"source":["# EVALUATE YOUR MODEL ON TRUE TESTING DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0n2sHcTVBuY"},"outputs":[],"source":["raise ValueError('do not go below this line!')"]},{"cell_type":"code","source":["tree_depth = 20\n","node_split = 10\n","leaf_sample = 2\n","tree_clf = RandomForestClassifier(n_estimators = number_of_trees, \n","                          random_state = RAND_STATE,\n","                          min_samples_split = node_split,\n","                          min_samples_leaf = leaf_sample,\n","                          criterion = 'gini',\n","                          max_depth = tree_depth)\n","tree_clf.fit(train_features,train_labels)"],"metadata":{"id":"hlJI4wD-UpDq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HfrZtrKPVBuY"},"outputs":[],"source":["# # Read in data from url\n","from sklearn.metrics import f1_score\n","\n","url = \"https://raw.githubusercontent.com/eabarnes1010/course_ml_ats/main/data/fccwx_data_2021.csv\"\n","data = pd.read_csv(url,parse_dates=[\"Date\"],infer_datetime_format=True)\n","data['dayofyear'] = data['Date'].dt.dayofyear\n","data.reindex(index=data.index[::-1])\n","\n","# Labels are the values we want to predict\n","labels = np.zeros((np.shape(data[TARGET_VAR])))\n","i = np.where(data[TARGET_VAR] < THRESHOLD_TEMP)[0]\n","labels[i] = 1\n","i = np.where(data[TARGET_VAR] < -THRESHOLD_TEMP/2)[0]\n","labels[i] = 2\n","\n","# Remove the labels from the features\n","# axis 1 refers to the columns\n","features = data.drop(TARGET_VAR, axis = 1)\n","\n","# Also remove DewPt and Date\n","features = features.drop('DewPt [C]', axis = 1)   # comment out if you want the prediction task to be easy\n","features = features.drop('Date', axis = 1)\n","\n","# Saving feature names for later use\n","feature_list = list(features.columns)\n","\n","# Convert to numpy array\n","features = np.array(features)\n","\n","# make the predictions\n","y_pred_test = tree_clf.predict(features)\n","\n","# print the metrics report\n","print(sklearn.metrics.classification_report(labels,y_pred_test))\n","\n","# print final f1 score\n","print('---------------------------------------')\n","print('Macro F1-Score   : ' + str(f1_score(labels, y_pred_test,average='macro')))\n","print('Weighted F1-Score: ' + str(f1_score(labels, y_pred_test,average='weighted')))\n","\n","# print accuracies\n","print('---------------------------------------')\n","print('TRAINING ACCURACY  : ' + str(accuracy_score(train_labels, y_pred_train)))\n","print('VALIDATION ACCURACY: ' + str(accuracy_score(val_labels, y_pred_val)))\n","print('TESTING ACCURACY   : ' + str(accuracy_score(labels, y_pred_test)))\n","\n","plt.plot(features[:,-1],np.array(data[TARGET_VAR]),'.')\n","plt.plot(features[:,-1],(y_pred_test*50)-25,'.r')\n","plt.axhline(y=THRESHOLD_TEMP,linestyle='--',color='k')\n","plt.axhline(y=-THRESHOLD_TEMP/2,linestyle='--',color='k')\n","plt.xlabel('day of year')\n","plt.ylabel(TARGET_VAR)\n","plt.title('Testing Year 2021')\n","plt.show()"]},{"cell_type":"code","source":[""],"metadata":{"id":"wN08umcfQihc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"3wxDvrDmVu7C"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"tree_classifier_christman_hourly_competition.ipynb","provenance":[{"file_id":"13Xx2bBWJToNi1YEv_q7p67xEwG0J-n8O","timestamp":1644843366282}],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":0}