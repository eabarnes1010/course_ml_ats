{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNYOWfvEbWzO"
   },
   "source": [
    "# Feed-forward neural network using ozone data at Joshua Tree\n",
    "[![Latest release](https://badgen.net/github/release/Naereen/Strapdown.js)](https://github.com/eabarnes1010/course_ml_ats/tree/main/code)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eabarnes1010/course_ml_ats/blob/main/code/ann_ozone_joshuatree_blank.ipynb)\n",
    "\n",
    "* Created originally by TA Jamin Rader [CSU] for ATS 780A7 at Colorado State University led by Prof. Elizabeth Barnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sVKVYstVW_p"
   },
   "source": [
    "# 0. Set Up Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1646243894542,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "3ADJn5sDvJAU",
    "outputId": "67c1e407-6999-4fc1-97ea-cbde56593352"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "print(\"IN_COLAB = \" + str(IN_COLAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5519,
     "status": "ok",
     "timestamp": 1646243900028,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "xWEMpofqvMuN",
    "outputId": "94a2d758-d433-4c6c-8d43-010393858be8"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "\n",
    "# import pydot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# tf.compat.v1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1646243900029,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "uQ8qXrMovQFj",
    "outputId": "c7caf192-17af-42df-e577-0e925034bde4"
   },
   "outputs": [],
   "source": [
    "print(f\"python version = {sys.version}\")\n",
    "print(f\"numpy version = {np.__version__}\")\n",
    "print(f\"tensorflow version = {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1836,
     "status": "ok",
     "timestamp": 1646243901824,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "n-3QsgoIvSOr"
   },
   "outputs": [],
   "source": [
    "## UNCOMMENT IF YOU WANT TO SAVE FIGURES IN COLABORATORY\n",
    "\n",
    "# if(IN_COLAB==True):\n",
    "#     try:\n",
    "#         from google.colab import drive\n",
    "#         drive.mount('/content/drive', force_remount=True)\n",
    "#         local_path = '/content/drive/My Drive/Colab Notebooks/'\n",
    "#     except:\n",
    "#         local_path = './'\n",
    "# else:\n",
    "#     local_path = '../figures/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYzCs4hS0ShZ"
   },
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ts-S49oV0In4"
   },
   "source": [
    "### 1.1 Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCosPTs30iFf"
   },
   "source": [
    "This is ozone and meteorlogical data from [CASTNET](https://www.epa.gov/castnet) (Clean Air Status and Trends Network) for Joshua Tree National Park, located just outside of Palm Springs and about 100 miles east of Los Angeles. The National Park Service monitors ozone in their parks. Joshua Tree has recorded at least 30 exceedance days per year [since 2016](https://www.nps.gov/subjects/air/ozone-exceed.htm). An exceedance day occurs when the daily maximum 8-hour ozone average is 71 ppb or higher. For comparison, Rocky Mountain NP has only experienced 35 ozone exceedance days since 2016.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25100,
     "status": "ok",
     "timestamp": 1646243926891,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "vrDpNnArxE5V"
   },
   "outputs": [],
   "source": [
    "# Read in data from url\n",
    "url = \"https://raw.githubusercontent.com/eabarnes1010/course_ml_ats/main/data/ozone_data_joshuatreenp.csv\"\n",
    "data = pd.read_csv(url, parse_dates=[\"DATE_TIME\"], infer_datetime_format=True)\n",
    "\n",
    "# Fix data issue with Daylight Savings Time\n",
    "duplicate_dates = data[\"DATE_TIME\"][data.duplicated(\"DATE_TIME\")]\n",
    "for dup_date in duplicate_dates:\n",
    "    idx = data[\"DATE_TIME\"].eq(dup_date).idxmax()\n",
    "    data.at[idx, \"DATE_TIME\"] = dup_date - pd.Timedelta(value=1, unit=\"hours\")\n",
    "\n",
    "# Add hour and day of year\n",
    "data[\"HOUR\"] = data[\"DATE_TIME\"].dt.hour\n",
    "data[\"MONTH\"] = data[\"DATE_TIME\"].dt.month\n",
    "data[\"YEAR\"] = data[\"DATE_TIME\"].dt.year\n",
    "data[\"DAYOFYEAR\"] = data[\"DATE_TIME\"].dt.dayofyear\n",
    "data.sort_values(\"DATE_TIME\", inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfcWmi6-odBK"
   },
   "source": [
    "Let's take a look at the data. We have data for ozone, temperature, relative humidity, and wind direction, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1646243926925,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "a7gFEvuuCQo_",
    "outputId": "d62493b0-f293-4077-c6ab-179cdcca8119"
   },
   "outputs": [],
   "source": [
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcts6nWEo1bZ"
   },
   "source": [
    "### 1.2 Define Input and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCegAFcPrvKZ"
   },
   "source": [
    "The 2015 benchmark for [human health ozone condition](https://www.nps.gov/articles/analysis-methods2020.htm) is shown here. Let us predict whether the ozone quality will be classified as good, fair, or poor over 8-hour periods.\n",
    "\n",
    "**Good**   $\\leq$ 54.9 ppb\n",
    "\n",
    "**Fair**   55.0 - 70.9 ppb\n",
    "\n",
    "**Poor**   $\\geq$ 71.0 ppb\n",
    "\n",
    "Let's start out by training our model using Temperature, Relative Humidity, Windspeed, and Day of Year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1646243926926,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "zraOwSRgsf6f",
    "outputId": "cd8973ce-1372-4e86-b0b7-57588fc892de"
   },
   "outputs": [],
   "source": [
    "# Here are all the different variables that we could use for training our neural\n",
    "# nework (except ozone, of course)\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd5gJJUCsWjS"
   },
   "source": [
    "**EDIT the Input Variables Here:** Reminder, if you choose to use wind direction, you must first convert it to a vector for averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1646243926927,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "hMoAfkADsSFo"
   },
   "outputs": [],
   "source": [
    "# List of strings from the available column names in the data set\n",
    "INPUT_VARIABLES = [\n",
    "    \"TEMPERATURE\",\n",
    "    \"RELATIVE_HUMIDITY\",\n",
    "    \"WINDSPEED\",\n",
    "    \"DAYOFYEAR\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1646243927121,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "u9LjpXXgC3zd",
    "outputId": "43d1b5af-326c-4ea2-9803-9a0a56fcf333"
   },
   "outputs": [],
   "source": [
    "# Let's isolate our variables of interest and take the 8-hour running mean\n",
    "\n",
    "# First using input and output variables together to take running mean\n",
    "df_data_to_be_used = data[[\"OZONE\"] + INPUT_VARIABLES]\n",
    "\n",
    "# Here we take the 8-hour rolling mean (note: DATE_TIME does not work)\n",
    "df_data_to_be_used = df_data_to_be_used.rolling(8).mean()\n",
    "\n",
    "# Now adding Date and Time components\n",
    "df_data_to_be_used[[\"DATE_TIME\", \"HOUR\", \"MONTH\", \"YEAR\"]] = data[[\"DATE_TIME\", \"HOUR\", \"MONTH\", \"YEAR\"]]\n",
    "\n",
    "# Dropping NaNs\n",
    "df_data_to_be_used.dropna(inplace=True)\n",
    "\n",
    "display(df_data_to_be_used.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1646243927121,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "PcO80t7pJKgw",
    "outputId": "28c79744-b4f9-4a6a-b5f8-39ec6a0c4bb2"
   },
   "outputs": [],
   "source": [
    "# Creating a numpy array for our inputs and outputs\n",
    "input = df_data_to_be_used[INPUT_VARIABLES].values\n",
    "output_raw = df_data_to_be_used[\"OZONE\"].values\n",
    "\n",
    "# Creating numpy arrays for time/date info for visualizations\n",
    "hour = df_data_to_be_used[\"HOUR\"].values\n",
    "month = df_data_to_be_used[\"MONTH\"].values\n",
    "year = df_data_to_be_used[\"YEAR\"].values\n",
    "\n",
    "# Turning ozone into classification problem:\n",
    "# Class 0: Good, Class 1: Fair, Class 2: Poor\n",
    "output_class = (output_raw >= 55).astype(int) + (output_raw >= 71).astype(int)\n",
    "output = (output_class.reshape(-1, 1) == np.unique(output_class)).astype(int)\n",
    "\n",
    "# Here are some examples of how our data is encoded into classes.\n",
    "print(\"Ozone Value:\", output_raw[0])\n",
    "print(\"Ozone Class:\", output_class[0])\n",
    "print(\"Encoded As:\", output[0])\n",
    "print()\n",
    "print(\"Ozone Value:\", output_raw[2000])\n",
    "print(\"Ozone Class:\", output_class[2000])\n",
    "print(\"Encoded As:\", output[2000])\n",
    "print()\n",
    "print(\"Ozone Value:\", output_raw[90116])\n",
    "print(\"Ozone Class:\", output_class[90116])\n",
    "print(\"Encoded As:\", output[90116])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1646243927298,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "LjiBWXuw2Qbo",
    "outputId": "1c049d4f-6e37-46fe-f31c-744cc1bb242c"
   },
   "outputs": [],
   "source": [
    "# Printing the shapes of our input and output arrays (#samples , #dimension of input/output)\n",
    "print(\"Input Array Shape:\", input.shape)\n",
    "print(\"Output Array Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lrkzNsd3vPY"
   },
   "source": [
    "### 1.3 Visualizing our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saw7iTsVVQJp"
   },
   "source": [
    "Let's look at what our output data actually looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1646243927299,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "tO_45vnhDeDq",
    "outputId": "cdccc31d-72c8-457e-b52b-89750d7732e5"
   },
   "outputs": [],
   "source": [
    "# How often does our data fall into each category?\n",
    "calcpercent = lambda cat: str((np.sum(output_class == cat) / len(output_class) * 100).astype(int))\n",
    "\n",
    "# Print out the sizes of each class\n",
    "print(\"Frequency for each Ozone Category\")\n",
    "print(\"Good: \" + calcpercent(0) + \"%\")\n",
    "print(\"Fair: \" + calcpercent(1) + \"%\")\n",
    "print(\"Poor: \" + calcpercent(2) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1646243927922,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "6AjbZz3Y3ufo",
    "outputId": "5f38af23-583b-4e30-9b9f-4eda3e589b8d"
   },
   "outputs": [],
   "source": [
    "# Distribution of ozone concentrations\n",
    "sb.displot(output_raw, kind=\"hist\")\n",
    "plt.xlabel(\"Ozone (ppb)\")\n",
    "plt.axvline(x=71, color=\"red\")\n",
    "plt.axvline(x=55, color=\"goldenrod\")\n",
    "plt.text(56, 50, \"Fair\", rotation=90, color=\"goldenrod\")\n",
    "plt.text(72, 50, \"Poor\", rotation=90, color=\"red\")\n",
    "\n",
    "plt.title(\"Histogram of O3 Concentrations in Joshua Tree NP\", fontweight=\"demi\")\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 4227,
     "status": "ok",
     "timestamp": 1646243932133,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "Tp38o54I5ESG",
    "outputId": "77cd6fef-2c66-4ee6-b9bd-517c890c923c"
   },
   "outputs": [],
   "source": [
    "# Distribution of ozone concentrations in each month\n",
    "\n",
    "fig, axs = plt.subplots(2, 6, figsize=(15, 5))\n",
    "\n",
    "for m in np.arange(12):\n",
    "    ax = axs[m // 6, m % 6]\n",
    "    sb.histplot(output_raw[month == m + 1], ax=ax)\n",
    "    ax.set_title(datetime.datetime.strptime(str(m + 1), \"%m\").strftime(\"%b\"))\n",
    "    ax.axvline(x=71, color=\"red\")\n",
    "    ax.axvline(x=55, color=\"goldenrod\")\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 500)\n",
    "\n",
    "fig.tight_layout(pad=1.0)\n",
    "print(\"O3 Concentrations in Each Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 4103,
     "status": "ok",
     "timestamp": 1646243936218,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "vqdk4zfY-ACv",
    "outputId": "80a561f3-f681-4752-afd3-775254c30ffb"
   },
   "outputs": [],
   "source": [
    "# Distribution of ozone concentrations in each year\n",
    "\n",
    "fig, axs = plt.subplots(2, 7, figsize=(15, 5))\n",
    "\n",
    "axidx = 0\n",
    "for y in np.unique(year):\n",
    "    ax = axs[axidx // 7, axidx % 7]\n",
    "    sb.histplot(output_raw[year == y], ax=ax)\n",
    "    ax.set_title(y)\n",
    "    ax.axvline(x=71, color=\"red\")\n",
    "    ax.axvline(x=55, color=\"goldenrod\")\n",
    "    ax.set_xlim(0, 100)\n",
    "    axidx += 1\n",
    "\n",
    "fig.tight_layout(pad=1.0)\n",
    "print(\"O3 Concentrations in Each Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1646243936794,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "O7KyKo2nCkMS",
    "outputId": "a0d47450-4101-4ab1-83d2-e8a28654c1e4"
   },
   "outputs": [],
   "source": [
    "# How many samples are available in each year? Data cannot be used if\n",
    "# there are NaNs (see 2013)\n",
    "sb.histplot(year)\n",
    "plt.title(\"Number of Usable Samples in Each Year\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84Z-QzHQ3Gos"
   },
   "source": [
    "### 1.4 Partitioning Data in Training, Validation, and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZzjby-W137P"
   },
   "source": [
    "Our data is highly temporally correlated, so we are going to separate training, validation, and testing by grabbing different years of data. *Not* by random sampling.\n",
    "\n",
    "**Some Variable Definitions**\n",
    "\n",
    "* ```Xtrain/Xval/Xtest:*** 2-D Arrays of input data (shape: #samples, #input_variables)```\n",
    "\n",
    "* ```Ttrain/Tval/Ttest:*** 2-D Arrays of target output data (true ozone class likelihood; shape: #samples, #classes)```\n",
    "\n",
    "* ```Ptrain/Pval/Ptest:*** 2-D Arrays of predicted output data (predicted ozone class likelihoods; shape: #samples, #classes)```\n",
    "\n",
    "* ```Xtrain_raw/Xval_raw/Xtest_raw:*** 2-D Arrays of raw (pre-standardized) input data (shape: #samples, #input_variables)```\n",
    "\n",
    "* ```O3train/O3val/O3test:*** 1-D Arrays of raw ozone measurements (ppb; shape: #samples)```\n",
    "\n",
    "* ```Cttrain/Ctval/Cttest:*** 1-D Arrays of the true ozone class (shape: #samples)```\n",
    "\n",
    "* ```Cptrain/Cpval/Cptest:*** 1-D Arrays of the predicted ozone class with the highest likelihood (shape #samples)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bD_e5dU9xgA4"
   },
   "source": [
    "**EDIT the years used for training, validation and testing here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1646243936794,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "qS34wb-2xSUV"
   },
   "outputs": [],
   "source": [
    "# Using the years 2010 - 2017 for training, 2018-2019 for validation, and 2020-2021 for testing\n",
    "TRAIN_RANGE = (2010, 2017)\n",
    "VAL_RANGE = (2018, 2019)\n",
    "TEST_RANGE = (2020, 2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1646243936795,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "rdaEo2W8Jfv4",
    "outputId": "ce6b36a7-5aa1-4543-efb9-74b2048fd5c5"
   },
   "outputs": [],
   "source": [
    "# Splitting into training, testing, validation\n",
    "\n",
    "# This function returns a boolean array of years that fall within the given year range\n",
    "year_bool = lambda yrrange: np.logical_and(year >= yrrange[0], year <= yrrange[1])\n",
    "\n",
    "# Create the input and output arrays from training, testing, validation sets\n",
    "# Inputs haven't been standardized yet (thus \"_raw\")\n",
    "\n",
    "Xtrain_raw = input[year_bool(TRAIN_RANGE)]  # these are the inputs (X)\n",
    "Ttrain = output[year_bool(TRAIN_RANGE)]  # these are the outputs (T is for target)\n",
    "\n",
    "Xval_raw = input[year_bool(VAL_RANGE)]\n",
    "Tval = output[year_bool(VAL_RANGE)]\n",
    "\n",
    "Xtest_raw = input[year_bool(TEST_RANGE)]\n",
    "Ttest = output[year_bool(TEST_RANGE)]\n",
    "\n",
    "# These are the raw outputs in each set for use later\n",
    "O3train = output_raw[year_bool(TRAIN_RANGE)]\n",
    "O3val = output_raw[year_bool(VAL_RANGE)]\n",
    "O3test = output_raw[year_bool(TEST_RANGE)]\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  Xtrain: \", Xtrain_raw.shape)\n",
    "print(\"  Xval: \", Xval_raw.shape)\n",
    "print(\"  Xtest: \", Xtest_raw.shape)\n",
    "print(\"  Ttrain: \", Ttrain.shape)\n",
    "print(\"  Tval: \", Tval.shape)\n",
    "print(\"  Ttest: \", Ttest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1646243936795,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "14W1NsXFWWLy"
   },
   "outputs": [],
   "source": [
    "# Standardizing the training, testing, and validation data\n",
    "\n",
    "# This function takes a raw set of input fields (for example, the training,\n",
    "# validation, or testing arrays), and standardizes it based on the training data.\n",
    "\n",
    "standardize_input = lambda dat, x, s: (dat - x) / s\n",
    "\n",
    "# Calculate mean and standard deviation of the training data\n",
    "trainmean = Xtrain_raw.mean(axis=0)\n",
    "trainstd = Xtrain_raw.std(axis=0)\n",
    "\n",
    "Xtrain = standardize_input(Xtrain_raw, trainmean, trainstd)\n",
    "Xval = standardize_input(Xval_raw, trainmean, trainstd)\n",
    "Xtest = standardize_input(Xtest_raw, trainmean, trainstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop here.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sI3_qEFUKkjb"
   },
   "source": [
    "# 2. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWXnTtfnKxnG"
   },
   "source": [
    "### 2.1 Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train, settings):\n",
    "    # create input layer\n",
    "    input_layer = tf.keras.layers.Input(shape=x_train.shape[1:])\n",
    "\n",
    "    # create a normalization layer if you would like\n",
    "    normalizer = tf.keras.layers.Normalization(axis=(1,))\n",
    "    normalizer.adapt(x_train)\n",
    "    layers = normalizer(input_layer)\n",
    "\n",
    "    # create hidden layers each with specific number of nodes\n",
    "    assert len(settings[\"hiddens\"]) == len(\n",
    "        settings[\"activations\"]\n",
    "    ), \"hiddens and activations settings must be the same length.\"\n",
    "\n",
    "    # add dropout layer\n",
    "    layers = tf.keras.layers.Dropout(rate=settings[\"dropout_rate\"])(layers)\n",
    "\n",
    "    for hidden, activation in zip(settings[\"hiddens\"], settings[\"activations\"]):\n",
    "        layers = tf.keras.layers.Dense(\n",
    "            units=hidden,\n",
    "            activation=activation,\n",
    "            use_bias=True,\n",
    "            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0),\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"]),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"]),\n",
    "        )(layers)\n",
    "\n",
    "    # create output layer\n",
    "    output_layer = tf.keras.layers.Dense(\n",
    "        units=y_train.shape[-1],\n",
    "        activation=\"softmax\",\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"] + 1),\n",
    "        kernel_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"] + 2),\n",
    "    )(layers)\n",
    "\n",
    "    # construct the model\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model, settings):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=settings[\"learning_rate\"]),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(),\n",
    "        ],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"hiddens\": [3, 3],\n",
    "    \"activations\": [\"relu\", \"relu\"],\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"random_seed\": 33,\n",
    "    \"max_epochs\": 1_000,\n",
    "    \"batch_size\": 256,\n",
    "    \"patience\": 10,\n",
    "}\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(settings[\"random_seed\"])\n",
    "\n",
    "model = build_model(Xtrain, Ttrain, settings)\n",
    "model = compile_model(model, settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13wObSJaoSsk"
   },
   "source": [
    "### 2.2 Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the class weights\n",
    "class_weights = {\n",
    "    0: 1 / np.mean(Ttrain[:, 0] == 1),\n",
    "    1: 1 / np.mean(Ttrain[:, 1] == 1),\n",
    "    2: 1 / np.mean(Ttrain[:, 2] == 1),\n",
    "}\n",
    "# class_weights = {0: 1, 1: 1, 2: 1}\n",
    "\n",
    "# define the early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=settings[\"patience\"], restore_best_weights=True, mode=\"auto\"\n",
    ")\n",
    "\n",
    "# train the model via model.fit\n",
    "history = model.fit(\n",
    "    Xtrain,\n",
    "    Ttrain,\n",
    "    epochs=settings[\"max_epochs\"],\n",
    "    batch_size=settings[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    validation_data=[Xval, Tval],\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQT3clZbZDXn"
   },
   "source": [
    "# 3. Model Performance\n",
    "How should we evaluate our ANN's performance? Categorical accuracy is one way, which tells us how often any class was correctly predicted. However, by this metric, since 65% of our data points are from days with \"Good\" ozone, our model could learn to predict \"Good\" every time and we would still be 65% accurate. This doesn't allow us to learn anything about what it takes to predict \"Fair\" or \"Poor\" ozone days. Below, we compare categorical accuracy, to weighted categorical accuracy which takes into account class imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1646243960774,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "kGV1eFO56uXm",
    "outputId": "45786bd8-3b3b-41bc-8d15-89b60ecbcff9"
   },
   "outputs": [],
   "source": [
    "# Let's plot the change in loss and categorical_accuracy\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axs[0].plot(history.history[\"loss\"], label=\"training\")\n",
    "axs[0].plot(history.history[\"val_loss\"], label=\"validation\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(history.history[\"categorical_accuracy\"], label=\"training\")\n",
    "axs[1].plot(history.history[\"val_categorical_accuracy\"], label=\"validation\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Categorical Accuracy\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1578,
     "status": "ok",
     "timestamp": 1646243962334,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "0ry9E6bJpAkU",
    "outputId": "92ff0f43-2a17-4c60-bcbe-372339fe7b34"
   },
   "outputs": [],
   "source": [
    "# What predictions did the model make for our training, validation, and test sets?\n",
    "Ptrain = model.predict(Xtrain)  # Array of class likelihoods for each class\n",
    "Pval = model.predict(Xval)\n",
    "Ptest = model.predict(Xtest)\n",
    "\n",
    "Cptrain = Ptrain.argmax(axis=1)  # 1-D array of predicted class (highest likelihood)\n",
    "Cpval = Pval.argmax(axis=1)\n",
    "Cptest = Ptest.argmax(axis=1)\n",
    "\n",
    "Cttrain = Ttrain.argmax(axis=1)  # 1-D array of truth class\n",
    "Ctval = Tval.argmax(axis=1)\n",
    "Cttest = Ttest.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1646243962500,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "F5mjzp1EjRPo",
    "outputId": "c70f4305-757c-41e9-9f48-434757eef7a9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "print(\"Validation Categorical Accuracy:\", accuracy_score(Ctval, Cpval))\n",
    "\n",
    "# Weight equal to the inverse of the frequency of the class\n",
    "cat_weights = np.sum((1 / np.mean(Ttrain, axis=0)) * Tval, axis=1)\n",
    "print(\"Validation Weighted Categorical Accuracy:\", accuracy_score(Ctval, Cpval, sample_weight=cat_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1646243962500,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "capsBG1d1uW0"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(predclasses, targclasses):\n",
    "    class_names = np.unique(targclasses)\n",
    "\n",
    "    table = []\n",
    "    for pred_class in class_names:\n",
    "        row = []\n",
    "        for true_class in class_names:\n",
    "            row.append(100 * np.mean(predclasses[targclasses == true_class] == pred_class))\n",
    "        table.append(row)\n",
    "    class_titles_t = [\"T(Good)\", \"T(Fair)\", \"T(Poor)\"]\n",
    "    class_titles_p = [\"P(Good)\", \"P(Fair)\", \"P(Poor)\"]\n",
    "    conf_matrix = pd.DataFrame(table, index=class_titles_p, columns=class_titles_t)\n",
    "    display(conf_matrix.style.background_gradient(cmap=\"Blues\").format(\"{:.1f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1646243962655,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "8JZegNvJ1x6-",
    "outputId": "ab98ad03-a169-4d18-fbf5-c8355549aea6"
   },
   "outputs": [],
   "source": [
    "print(\"Predicted versus Target Classes\")\n",
    "confusion_matrix(Cptrain, Cttrain)\n",
    "confusion_matrix(Cpval, Ctval)\n",
    "# confusion_matrix(Cptest, Cttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1646243962988,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "oLyRWUOu4sMN",
    "outputId": "90774ebe-8ae4-4423-ccf6-2a2ed3cb9a43"
   },
   "outputs": [],
   "source": [
    "df_class0 = pd.DataFrame(O3val[Cpval == 0])\n",
    "df_class1 = pd.DataFrame(O3val[Cpval == 1])\n",
    "df_class2 = pd.DataFrame(O3val[Cpval == 2])\n",
    "\n",
    "sb.violinplot(data=[O3val[Cpval == 0], O3val[Cpval == 1], O3val[Cpval == 2]])\n",
    "\n",
    "plt.axhline(55, color=\"goldenrod\", zorder=0)\n",
    "plt.axhline(71, color=\"red\", zorder=0)\n",
    "plt.ylabel(\"Ozone (ppb)\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.xticks(\n",
    "    [0, 1, 2],\n",
    "    [\"Good\", \"Fair\", \"Poor\"],\n",
    ")\n",
    "plt.title(\"O3 Concentrations when each Class is Predicted\", fontweight=\"demi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop code prior to explainability techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlQ7lQs6qMab"
   },
   "source": [
    "# 4. Explainability Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjh2w71OEblQ"
   },
   "source": [
    "### 4.1 SHAP (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMrVhbdyajwr"
   },
   "source": [
    "We know the accuracy of our neural network. When it makes correct predictions, is it making the right prediction for the right reasons? This is where explainable AI comes in.\n",
    "\n",
    "Below we will use DeepSHAP to understand how our neural network made its predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 7119,
     "status": "ok",
     "timestamp": 1646243970087,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "mizWJ7op7wA0",
    "outputId": "fb1abd93-f133-4c69-affe-324c778976d0"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Installing SHAP\n",
    "    %pip install shap\n",
    "\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52288,
     "status": "ok",
     "timestamp": 1646244022314,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "6a5bOjdyQ-up",
    "outputId": "715d6b9b-e49a-41b8-d803-78a25fe58695"
   },
   "outputs": [],
   "source": [
    "# DeepSHAP takes a long time, so we make it easier by letting it learn the\n",
    "# relationships between the features in a subset of the training set.\n",
    "background = Xtrain[np.random.choice(Xtrain.shape[0], 100, replace=False)]\n",
    "\n",
    "# We will use this to explain the neural network's decision-making process\n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "\n",
    "# Selecting data for when a prediction was correct\n",
    "Xgoodgood = Xtest[np.logical_and(Cttest == 0, Cptest == 0)]\n",
    "Xfairfair = Xtest[np.logical_and(Cttest == 1, Cptest == 1)]\n",
    "Xpoorpoor = Xtest[np.logical_and(Cttest == 2, Cptest == 2)]\n",
    "\n",
    "# These are the SHAP values for when a prediction was correct\n",
    "shap_values_goodgood = explainer.shap_values(Xgoodgood)[0]\n",
    "shap_values_fairfair = explainer.shap_values(Xfairfair)[1]\n",
    "shap_values_poorpoor = explainer.shap_values(Xpoorpoor)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1646244022330,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "z_3XNmc5RFHU"
   },
   "outputs": [],
   "source": [
    "# Reduce noise by taking means of sorted 1% segments of the data\n",
    "def get_percentile_stats(arr, sortarr, num=100, func=np.mean):\n",
    "    sorted_arr = arr[np.argsort(sortarr)]  # Sorts arr based on values in sortarr\n",
    "    split_arrs = np.array_split(sorted_arr, num)  # Splits the array into [num] lists\n",
    "    meanslist = list(map(func, split_arrs))  # Applies np.mean to all arrays in split_arrs\n",
    "    return np.array(meanslist)  # Returns array of the means for [num] sorted segments of arr\n",
    "\n",
    "\n",
    "# Plot out the Shapley Values in a more visually appealing format\n",
    "def plot_shap(shapvals, featurevals, title=False):\n",
    "    # Number of samples\n",
    "    samp_num = shapvals.shape[0]\n",
    "\n",
    "    # Init colormap\n",
    "    n = len(INPUT_VARIABLES)\n",
    "    color = iter(plt.cm.get_cmap(\"viridis\")(np.linspace(0, 1, n)))\n",
    "\n",
    "    for varindex, varname in enumerate(INPUT_VARIABLES):\n",
    "        # Step color\n",
    "        c = next(color)\n",
    "\n",
    "        # Get the avg feature val for every 5 percentiles of shap values\n",
    "        featuremean_for_shappercentile = get_percentile_stats(featurevals[:, varindex], shapvals[:, varindex])\n",
    "\n",
    "        # Get the median shap val for every 5 percentiles of shap values\n",
    "        shapmedian_for_shappercentile = get_percentile_stats(\n",
    "            shapvals[:, varindex], shapvals[:, varindex], func=np.median\n",
    "        )\n",
    "\n",
    "        # Plot\n",
    "        plt.plot(featuremean_for_shappercentile, shapmedian_for_shappercentile, \"o\", label=varname, color=c)\n",
    "        plt.axhline(0, zorder=0, color=\"k\", alpha=0.1)\n",
    "        plt.axvline(0, zorder=0, color=\"k\", alpha=0.1)\n",
    "\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.legend(bbox_to_anchor=(1.6, 1), loc=\"upper right\")\n",
    "\n",
    "    plt.ylabel(\"Median Shap Value\")\n",
    "    plt.xlabel(\"Mean Feature Value\")\n",
    "    if title:\n",
    "        plt.title(\n",
    "            \"SHAP Values and Feature Values when \\nNeural Network correctly \"\n",
    "            + \"predicts '\"\n",
    "            + title\n",
    "            + \"' Ozone\\nN=\"\n",
    "            + str(samp_num)\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blTLeDZTgzx7"
   },
   "source": [
    "The plots below are de-noised versions of all our SHAP values. They tell us two things:\n",
    "\n",
    "1) When a certain class of ozone was correctly predicted, what did the feature values look like? For example, when a 'Good' day was correctly predicted, were the temperature values generally warmer or cooler than average (data is standardized, so the average is zero).\n",
    "\n",
    "2) When a certain class of ozone was correctly predicted, how did changes in the feature values allow the network to be more certain (positive SHAP values) or less certain (negative SHAP values) in its prediction? For example, when windspeed was higher did this make the neural network more confident in predicting a 'Poor' ozone day?\n",
    "\n",
    "See what you can learn about the relationship between ozone and the input features by looking at these plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "executionInfo": {
     "elapsed": 1491,
     "status": "ok",
     "timestamp": 1646244023786,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "IG0_Gn7nRH2O",
    "outputId": "c8f6ca26-ce73-42c5-9ef9-68be5f4c458a"
   },
   "outputs": [],
   "source": [
    "plot_shap(shap_values_goodgood, Xgoodgood, title=\"Good\")\n",
    "plot_shap(shap_values_fairfair, Xfairfair, title=\"Fair\")\n",
    "plot_shap(shap_values_poorpoor, Xpoorpoor, title=\"Poor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7reR-uCQM7E",
    "tags": []
   },
   "source": [
    "# 5. Model Competition - Do Not Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqIVAjqyQUih",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "We have set aside a bunch of ozone data. Tune your model to the best of your abilities, and we will see how it performed at the end of class. Specifically, we will be using Weighted Categorical Accuracy to measure model performance. **EDIT the code below to test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1646244023787,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "NGEseP36QTL8"
   },
   "outputs": [],
   "source": [
    "CODE = \"\"  # We will give you this at the end of class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp3-WphNX8UZ"
   },
   "source": [
    "DO NOT EDIT THE FOLLOWING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1646244023787,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "SQuFhq_lRzNB"
   },
   "outputs": [],
   "source": [
    "def compete():\n",
    "    # Read in data from url\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/eabarnes1010/course_ml_ats/main/data/ozone_data_joshuatreenp_\"\n",
    "        + CODE\n",
    "        + \".csv\"\n",
    "    )\n",
    "    data = pd.read_csv(url, parse_dates=[\"DATE_TIME\"], infer_datetime_format=True)\n",
    "\n",
    "    # Fix data issue with Daylight Savings Time\n",
    "    duplicate_dates = data[\"DATE_TIME\"][data.duplicated(\"DATE_TIME\")]\n",
    "    for dup_date in duplicate_dates:\n",
    "        idx = data[\"DATE_TIME\"].eq(dup_date).idxmax()\n",
    "        data.at[idx, \"DATE_TIME\"] = dup_date - pd.Timedelta(value=1, unit=\"hours\")\n",
    "\n",
    "    # Add hour and day of year\n",
    "    data[\"HOUR\"] = data[\"DATE_TIME\"].dt.hour\n",
    "    data[\"MONTH\"] = data[\"DATE_TIME\"].dt.month\n",
    "    data[\"YEAR\"] = data[\"DATE_TIME\"].dt.year\n",
    "    data[\"DAYOFYEAR\"] = data[\"DATE_TIME\"].dt.dayofyear\n",
    "    data.sort_values(\"DATE_TIME\", inplace=True, ignore_index=True)\n",
    "\n",
    "    df_data_to_be_used = data[[\"OZONE\"] + INPUT_VARIABLES]\n",
    "    df_data_to_be_used = df_data_to_be_used.rolling(8).mean()\n",
    "    df_data_to_be_used[[\"DATE_TIME\", \"HOUR\", \"MONTH\", \"YEAR\"]] = data[[\"DATE_TIME\", \"HOUR\", \"MONTH\", \"YEAR\"]]\n",
    "    df_data_to_be_used.dropna(inplace=True)\n",
    "\n",
    "    Xcompete_raw = df_data_to_be_used[INPUT_VARIABLES].values\n",
    "    output_raw = df_data_to_be_used[\"OZONE\"].values\n",
    "    hour = df_data_to_be_used[\"HOUR\"].values\n",
    "    month = df_data_to_be_used[\"MONTH\"].values\n",
    "    year = df_data_to_be_used[\"YEAR\"].values\n",
    "\n",
    "    output_class = (output_raw >= 55).astype(int) + (output_raw >= 71).astype(int)\n",
    "    Tcompete = (output_class.reshape(-1, 1) == np.unique(output_class)).astype(int)\n",
    "    year_bool = lambda yrrange: np.logical_and(year >= yrrange[0], year <= yrrange[1])\n",
    "\n",
    "    standardize_input = lambda dat, x, s: (dat - x) / s\n",
    "    Xcompete = standardize_input(Xcompete_raw, trainmean, trainstd)\n",
    "\n",
    "    Pcompete = model.predict(Xcompete)\n",
    "    Cpcompete = Pcompete.argmax(axis=1)\n",
    "    Ctcompete = Tcompete.argmax(axis=1)\n",
    "\n",
    "    cat_weights = np.sum((1 / np.mean(Ttrain, axis=0)) * Tcompete, axis=1)\n",
    "    print(\n",
    "        \"Congrats! Your overall weighted categorical accuracy is:\",\n",
    "        accuracy_score(Ctcompete, Cpcompete, sample_weight=cat_weights),\n",
    "    )\n",
    "    print(\"Predicted versus Target Classes for Competition Data\")\n",
    "    confusion_matrix(Cpcompete, Ctcompete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "executionInfo": {
     "elapsed": 623,
     "status": "error",
     "timestamp": 1646244024375,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "eMy7TLscU82g",
    "outputId": "d4b3c5ca-7d6b-4c35-9c4f-39508aa0db2f"
   },
   "outputs": [],
   "source": [
    "compete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 158,
     "status": "aborted",
     "timestamp": 1646244024233,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "c0NUbJhqVAz-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xYzCs4hS0ShZ"
   ],
   "name": "ann_ozone_joshuatree.ipynb",
   "provenance": [
    {
     "file_id": "1GtH4FaaaqRewMY8KlM5DOhHQl1ZPdiWt",
     "timestamp": 1642113733706
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
